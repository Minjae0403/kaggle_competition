{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-29T04:46:32.149693Z","iopub.execute_input":"2023-06-29T04:46:32.150174Z","iopub.status.idle":"2023-06-29T04:46:32.222774Z","shell.execute_reply.started":"2023-06-29T04:46:32.150136Z","shell.execute_reply":"2023-06-29T04:46:32.221342Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/cafa-5-protein-function-prediction/sample_submission.tsv\n/kaggle/input/cafa-5-protein-function-prediction/IA.txt\n/kaggle/input/cafa-5-protein-function-prediction/Test (Targets)/testsuperset.fasta\n/kaggle/input/cafa-5-protein-function-prediction/Test (Targets)/testsuperset-taxon-list.tsv\n/kaggle/input/cafa-5-protein-function-prediction/Train/train_terms.tsv\n/kaggle/input/cafa-5-protein-function-prediction/Train/train_sequences.fasta\n/kaggle/input/cafa-5-protein-function-prediction/Train/train_taxonomy.tsv\n/kaggle/input/cafa-5-protein-function-prediction/Train/go-basic.obo\n/kaggle/input/2022-06-22/id_seq_term_tax.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"#seq_file에서 dataframe 형식으로 data 불러오기\nseq_file = \"/kaggle/input/cafa-5-protein-function-prediction/Train/train_sequences.fasta\"\nimport pandas as pd\nimport re\ndef read_fasta(file_path, columns) :\n    from Bio.SeqIO.FastaIO import SimpleFastaParser \n    with open(file_path) as fasta_file :  \n        records = [] # create empty list\n\n        for title, sequence in SimpleFastaParser(fasta_file):  #SimpleFastaParser Iterate over Fasta records as string tuples. For each record a tuple of two strings is returned: the FASTA title line (without the leading ‘>’ character),  and the sequence (with any whitespace removed). \n            record = []\n            title_splits=re.findall(r\"[\\w']+\", title) # Data cleaning is needed\n            record.append(title_splits[0])  #First values are ID (Append adds element to a list)\n            record.append(len(sequence)) #Second values are sequences lengths\n            sequence = \" \".join(sequence) #It converts into one line\n            record.append(sequence)#Third values are sequences\n            records.append(record)\n    return pd.DataFrame(records, columns = columns) #We have created a function that returns a dataframe\n\n#Now let's use this function by inserting in the first argument the file name (or file path if your working directory is different from where the fasta file is)        \n#And in the second one the names of columns\ndata = read_fasta(seq_file, columns=[\"id\",\"sequence_length\", \"sequence\"])\ndata","metadata":{"execution":{"iopub.status.busy":"2023-06-29T04:46:35.972936Z","iopub.execute_input":"2023-06-29T04:46:35.975066Z","iopub.status.idle":"2023-06-29T04:46:44.123233Z","shell.execute_reply.started":"2023-06-29T04:46:35.975019Z","shell.execute_reply":"2023-06-29T04:46:44.122350Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                id  sequence_length  \\\n0           P20536              218   \n1           O73864              354   \n2           O95231              258   \n3       A0A0B4J1F4              415   \n4           P54366              415   \n...            ...              ...   \n142241  A0A286YAI0              450   \n142242  A0A1D5NUC4              643   \n142243      Q5RGB0              448   \n142244  A0A2R8QMZ5              459   \n142245  A0A8I6GHU0              138   \n\n                                                 sequence  \n0       M N S V T V S H A P Y T I T Y H D D W E P V M ...  \n1       M T E Y R N F L L L F I T S L S V I Y P C T G ...  \n2       M R L S S S P P R G P Q Q L S S F G S V D W L ...  \n3       M G G E A G A D G P R G R V K S L G L V F E D ...  \n4       M V E T N S P P A G Y T L K R S P S D L G E Q ...  \n...                                                   ...  \n142241  M E T E V D D F P G K A S I F S Q V N P L Y S ...  \n142242  M S A A A S A E M I E T P P V L N F E E I D Y ...  \n142243  M A D K G P I L T S V I I F Y L S I G A A I F ...  \n142244  M G R K K I Q I T R I M D E R N R Q V T F T K ...  \n142245  H C I S S L K L T A F F K R S F L L S P E K H ...  \n\n[142246 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>sequence_length</th>\n      <th>sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>P20536</td>\n      <td>218</td>\n      <td>M N S V T V S H A P Y T I T Y H D D W E P V M ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>O73864</td>\n      <td>354</td>\n      <td>M T E Y R N F L L L F I T S L S V I Y P C T G ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>O95231</td>\n      <td>258</td>\n      <td>M R L S S S P P R G P Q Q L S S F G S V D W L ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A0A0B4J1F4</td>\n      <td>415</td>\n      <td>M G G E A G A D G P R G R V K S L G L V F E D ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>P54366</td>\n      <td>415</td>\n      <td>M V E T N S P P A G Y T L K R S P S D L G E Q ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>142241</th>\n      <td>A0A286YAI0</td>\n      <td>450</td>\n      <td>M E T E V D D F P G K A S I F S Q V N P L Y S ...</td>\n    </tr>\n    <tr>\n      <th>142242</th>\n      <td>A0A1D5NUC4</td>\n      <td>643</td>\n      <td>M S A A A S A E M I E T P P V L N F E E I D Y ...</td>\n    </tr>\n    <tr>\n      <th>142243</th>\n      <td>Q5RGB0</td>\n      <td>448</td>\n      <td>M A D K G P I L T S V I I F Y L S I G A A I F ...</td>\n    </tr>\n    <tr>\n      <th>142244</th>\n      <td>A0A2R8QMZ5</td>\n      <td>459</td>\n      <td>M G R K K I Q I T R I M D E R N R Q V T F T K ...</td>\n    </tr>\n    <tr>\n      <th>142245</th>\n      <td>A0A8I6GHU0</td>\n      <td>138</td>\n      <td>H C I S S L K L T A F F K R S F L L S P E K H ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>142246 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#index 1번의 id에 담겨있는 나머지 정보 불러오기\nfrom Bio import SeqIO\n\n# Open the FASTA file\nfasta_sequences = SeqIO.parse(open(seq_file),'fasta')\n\n# Loop over each sequence and print its ID and length\nfor fasta in fasta_sequences:\n    print(fasta)\n    print(\"Sequence ID:\", fasta.id)\n    print(\"Sequence length:\", len(fasta))\n    break","metadata":{"execution":{"iopub.status.busy":"2023-06-29T04:46:44.125016Z","iopub.execute_input":"2023-06-29T04:46:44.125573Z","iopub.status.idle":"2023-06-29T04:46:44.158332Z","shell.execute_reply.started":"2023-06-29T04:46:44.125540Z","shell.execute_reply":"2023-06-29T04:46:44.157386Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"ID: P20536\nName: P20536\nDescription: P20536 sp|P20536|UNG_VACCC Uracil-DNA glycosylase OS=Vaccinia virus (strain Copenhagen) OX=10249 GN=UNG PE=1 SV=1\nNumber of features: 0\nSeq('MNSVTVSHAPYTITYHDDWEPVMSQLVEFYNEVASWLLRDETSPIPDKFFIQLK...FIY')\nSequence ID: P20536\nSequence length: 218\n","output_type":"stream"}]},{"cell_type":"code","source":"#불러온 정보 쪼개기\nfrom Bio import SeqIO\n\n# Open the FASTA file\nfasta_sequences = SeqIO.parse(open(seq_file),'fasta')\nfor fasta in fasta_sequences:\n    print(fasta)\n    print(\"----------------------\")\n    print(\"Seqence ID:\", fasta.id)\n    print(\"----------------------\")\n    print(\"Seqence name:\", fasta.name)\n    print(\"----------------------\")\n    fasta.description = fasta.description.split(\" \")\n    #seqence 설명 된 부분 쪼개기. 특성을 얻고잫 하였으나 각 seq마다 위치가 달라서 가능할지 의문\n    #이걸 쪼개서 dataframe에 넣고 분류 기준으로 잡으면 좋을것 같다.\n    print(\"Seqence description:\", fasta.description[1:])\n    print(\"----------------------\")    \n    print(\"Seqence:\", repr(fasta.seq))\n    print(\"----------------------\")  \n    print(fasta.annotations)\n    break","metadata":{"execution":{"iopub.status.busy":"2023-06-29T04:46:44.159679Z","iopub.execute_input":"2023-06-29T04:46:44.160230Z","iopub.status.idle":"2023-06-29T04:46:44.171030Z","shell.execute_reply.started":"2023-06-29T04:46:44.160199Z","shell.execute_reply":"2023-06-29T04:46:44.169521Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"ID: P20536\nName: P20536\nDescription: P20536 sp|P20536|UNG_VACCC Uracil-DNA glycosylase OS=Vaccinia virus (strain Copenhagen) OX=10249 GN=UNG PE=1 SV=1\nNumber of features: 0\nSeq('MNSVTVSHAPYTITYHDDWEPVMSQLVEFYNEVASWLLRDETSPIPDKFFIQLK...FIY')\n----------------------\nSeqence ID: P20536\n----------------------\nSeqence name: P20536\n----------------------\nSeqence description: ['sp|P20536|UNG_VACCC', 'Uracil-DNA', 'glycosylase', 'OS=Vaccinia', 'virus', '(strain', 'Copenhagen)', 'OX=10249', 'GN=UNG', 'PE=1', 'SV=1']\n----------------------\nSeqence: Seq('MNSVTVSHAPYTITYHDDWEPVMSQLVEFYNEVASWLLRDETSPIPDKFFIQLK...FIY')\n----------------------\n{}\n","output_type":"stream"}]},{"cell_type":"code","source":"seq_df = data.drop(labels = ['sequence_length'], axis = 1, inplace = False)\n#id 및 seqence만 별도 출력\n#글자 사이 공백제거\nseq_df","metadata":{"execution":{"iopub.status.busy":"2023-06-29T04:46:44.173409Z","iopub.execute_input":"2023-06-29T04:46:44.174041Z","iopub.status.idle":"2023-06-29T04:46:44.207230Z","shell.execute_reply.started":"2023-06-29T04:46:44.174004Z","shell.execute_reply":"2023-06-29T04:46:44.205958Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                id                                           sequence\n0           P20536  M N S V T V S H A P Y T I T Y H D D W E P V M ...\n1           O73864  M T E Y R N F L L L F I T S L S V I Y P C T G ...\n2           O95231  M R L S S S P P R G P Q Q L S S F G S V D W L ...\n3       A0A0B4J1F4  M G G E A G A D G P R G R V K S L G L V F E D ...\n4           P54366  M V E T N S P P A G Y T L K R S P S D L G E Q ...\n...            ...                                                ...\n142241  A0A286YAI0  M E T E V D D F P G K A S I F S Q V N P L Y S ...\n142242  A0A1D5NUC4  M S A A A S A E M I E T P P V L N F E E I D Y ...\n142243      Q5RGB0  M A D K G P I L T S V I I F Y L S I G A A I F ...\n142244  A0A2R8QMZ5  M G R K K I Q I T R I M D E R N R Q V T F T K ...\n142245  A0A8I6GHU0  H C I S S L K L T A F F K R S F L L S P E K H ...\n\n[142246 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>P20536</td>\n      <td>M N S V T V S H A P Y T I T Y H D D W E P V M ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>O73864</td>\n      <td>M T E Y R N F L L L F I T S L S V I Y P C T G ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>O95231</td>\n      <td>M R L S S S P P R G P Q Q L S S F G S V D W L ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A0A0B4J1F4</td>\n      <td>M G G E A G A D G P R G R V K S L G L V F E D ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>P54366</td>\n      <td>M V E T N S P P A G Y T L K R S P S D L G E Q ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>142241</th>\n      <td>A0A286YAI0</td>\n      <td>M E T E V D D F P G K A S I F S Q V N P L Y S ...</td>\n    </tr>\n    <tr>\n      <th>142242</th>\n      <td>A0A1D5NUC4</td>\n      <td>M S A A A S A E M I E T P P V L N F E E I D Y ...</td>\n    </tr>\n    <tr>\n      <th>142243</th>\n      <td>Q5RGB0</td>\n      <td>M A D K G P I L T S V I I F Y L S I G A A I F ...</td>\n    </tr>\n    <tr>\n      <th>142244</th>\n      <td>A0A2R8QMZ5</td>\n      <td>M G R K K I Q I T R I M D E R N R Q V T F T K ...</td>\n    </tr>\n    <tr>\n      <th>142245</th>\n      <td>A0A8I6GHU0</td>\n      <td>H C I S S L K L T A F F K R S F L L S P E K H ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>142246 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nterm_file = \"/kaggle/input/cafa-5-protein-function-prediction/Train/train_terms.tsv\"\nterm_df = pd.read_csv(term_file, delimiter = '\\t')\nterm_df = term_df.rename(columns = {'EntryID':'id'})\nterm_df","metadata":{"execution":{"iopub.status.busy":"2023-06-29T04:46:44.292981Z","iopub.execute_input":"2023-06-29T04:46:44.293383Z","iopub.status.idle":"2023-06-29T04:46:49.260801Z","shell.execute_reply.started":"2023-06-29T04:46:44.293354Z","shell.execute_reply":"2023-06-29T04:46:49.259763Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                 id        term aspect\n0        A0A009IHW8  GO:0008152    BPO\n1        A0A009IHW8  GO:0034655    BPO\n2        A0A009IHW8  GO:0072523    BPO\n3        A0A009IHW8  GO:0044270    BPO\n4        A0A009IHW8  GO:0006753    BPO\n...             ...         ...    ...\n5363858      X5L565  GO:0050649    MFO\n5363859      X5L565  GO:0016491    MFO\n5363860      X5M5N0  GO:0005515    MFO\n5363861      X5M5N0  GO:0005488    MFO\n5363862      X5M5N0  GO:0003674    MFO\n\n[5363863 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>term</th>\n      <th>aspect</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A0A009IHW8</td>\n      <td>GO:0008152</td>\n      <td>BPO</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A0A009IHW8</td>\n      <td>GO:0034655</td>\n      <td>BPO</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A0A009IHW8</td>\n      <td>GO:0072523</td>\n      <td>BPO</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A0A009IHW8</td>\n      <td>GO:0044270</td>\n      <td>BPO</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A0A009IHW8</td>\n      <td>GO:0006753</td>\n      <td>BPO</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5363858</th>\n      <td>X5L565</td>\n      <td>GO:0050649</td>\n      <td>MFO</td>\n    </tr>\n    <tr>\n      <th>5363859</th>\n      <td>X5L565</td>\n      <td>GO:0016491</td>\n      <td>MFO</td>\n    </tr>\n    <tr>\n      <th>5363860</th>\n      <td>X5M5N0</td>\n      <td>GO:0005515</td>\n      <td>MFO</td>\n    </tr>\n    <tr>\n      <th>5363861</th>\n      <td>X5M5N0</td>\n      <td>GO:0005488</td>\n      <td>MFO</td>\n    </tr>\n    <tr>\n      <th>5363862</th>\n      <td>X5M5N0</td>\n      <td>GO:0003674</td>\n      <td>MFO</td>\n    </tr>\n  </tbody>\n</table>\n<p>5363863 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"term_df['term'].value_counts().head(10)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T04:46:50.429190Z","iopub.execute_input":"2023-06-29T04:46:50.429926Z","iopub.status.idle":"2023-06-29T04:46:51.612905Z","shell.execute_reply.started":"2023-06-29T04:46:50.429884Z","shell.execute_reply":"2023-06-29T04:46:51.611713Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"GO:0005575    92912\nGO:0008150    92210\nGO:0110165    91286\nGO:0003674    78637\nGO:0005622    70785\nGO:0009987    61293\nGO:0043226    60883\nGO:0043229    58315\nGO:0005488    57380\nGO:0043227    55452\nName: term, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"term_df[term_df['term']=='GO:0008152']","metadata":{"execution":{"iopub.status.busy":"2023-06-22T08:12:12.345137Z","iopub.execute_input":"2023-06-22T08:12:12.346965Z","iopub.status.idle":"2023-06-22T08:12:13.399674Z","shell.execute_reply.started":"2023-06-22T08:12:12.346884Z","shell.execute_reply":"2023-06-22T08:12:13.398120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"taxonomy_file = \"/kaggle/input/cafa-5-protein-function-prediction/Train/train_taxonomy.tsv\"\ntaxonomy_df = pd.read_csv(taxonomy_file, delimiter = '\\t')\ntaxonomy_df = taxonomy_df.rename(columns = {'EntryID':'id'})\ntaxonomy_df","metadata":{"execution":{"iopub.status.busy":"2023-06-29T04:55:07.716229Z","iopub.execute_input":"2023-06-29T04:55:07.717067Z","iopub.status.idle":"2023-06-29T04:55:07.856382Z","shell.execute_reply.started":"2023-06-29T04:55:07.717029Z","shell.execute_reply":"2023-06-29T04:55:07.855250Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                id  taxonomyID\n0           Q8IXT2        9606\n1           Q04418      559292\n2           A8DYA3        7227\n3           Q9UUI3      284812\n4           Q57ZS4      185431\n...            ...         ...\n142241      Q5TD07        9606\n142242      A8BB17        7955\n142243  A0A2R8QBB1        7955\n142244      P0CT72      284812\n142245      Q9NZ43        9606\n\n[142246 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>taxonomyID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Q8IXT2</td>\n      <td>9606</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Q04418</td>\n      <td>559292</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A8DYA3</td>\n      <td>7227</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Q9UUI3</td>\n      <td>284812</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Q57ZS4</td>\n      <td>185431</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>142241</th>\n      <td>Q5TD07</td>\n      <td>9606</td>\n    </tr>\n    <tr>\n      <th>142242</th>\n      <td>A8BB17</td>\n      <td>7955</td>\n    </tr>\n    <tr>\n      <th>142243</th>\n      <td>A0A2R8QBB1</td>\n      <td>7955</td>\n    </tr>\n    <tr>\n      <th>142244</th>\n      <td>P0CT72</td>\n      <td>284812</td>\n    </tr>\n    <tr>\n      <th>142245</th>\n      <td>Q9NZ43</td>\n      <td>9606</td>\n    </tr>\n  </tbody>\n</table>\n<p>142246 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"데이터 합치기","metadata":{}},{"cell_type":"code","source":"df1 = term_df\ndf2 = seq_df\n\n# df1과 df2를 id를 기준으로 합치기\nmerged_df = df1.merge(df2, on='id')\n\n# 같은 id와 seq를 가진 행들의 function 합치기\nmerged_df = merged_df.groupby(['id', 'sequence','aspect'])['term'].apply(lambda x: '/'.join(x)).reset_index()\n\nterm_list = merged_df['term'].str.split('/')\nfor j in range(len(term_list[0])):\n    column_name = 'term' + str(j+1)\n    merged_df[column_name] = [terms[j] if len(terms) > j else None for terms in term_list]\n    \nmerged_df = merged_df.drop(columns=['term'])\nmerged_df","metadata":{"execution":{"iopub.status.busy":"2023-06-29T04:50:35.376846Z","iopub.execute_input":"2023-06-29T04:50:35.377247Z","iopub.status.idle":"2023-06-29T04:51:11.893831Z","shell.execute_reply.started":"2023-06-29T04:50:35.377213Z","shell.execute_reply":"2023-06-29T04:51:11.892690Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                id                                           sequence aspect  \\\n0       A0A009IHW8  M S L E Q K K G A D I I S K I L Q I Q N S I G ...    BPO   \n1       A0A009IHW8  M S L E Q K K G A D I I S K I L Q I Q N S I G ...    MFO   \n2       A0A021WW32  M F Y E H I I L A K K G P L A R I W L A A H W ...    BPO   \n3       A0A021WW32  M F Y E H I I L A K K G P L A R I W L A A H W ...    CCO   \n4       A0A021WZA4  M K Y I N C T Q P A I D D F P R D L F S E A Q ...    CCO   \n...            ...                                                ...    ...   \n263754      X6RKS3  M A N L G C W M L V L F V A T W S D L G L C K ...    CCO   \n263755      X6RLN4  E V K G L F K S E N C P K V I S C E F A H N S ...    CCO   \n263756      X6RLP6  M A A P E Q P L A I S R G C T S S S S L S P P ...    CCO   \n263757      X6RLR1  M A G L T D L Q R L Q A R V E E L E R W V Y G ...    CCO   \n263758      X6RM59  M D R A A V A R V G A V A S A S V C A L V A G ...    CCO   \n\n             term1       term2       term3       term4       term5  \\\n0       GO:0008152  GO:0034655  GO:0072523  GO:0044270  GO:0006753   \n1       GO:0003674  GO:0003953  GO:0016787  GO:0016799  GO:0016798   \n2       GO:0048869  GO:0048856  GO:0022008  GO:0065007  GO:0007275   \n3       GO:0099086  GO:0000228  GO:0005622  GO:0043226  GO:0000792   \n4       GO:0071944  GO:0005575  GO:0110165  GO:0016020  GO:0005886   \n...            ...         ...         ...         ...         ...   \n263754  GO:0005622  GO:0031090  GO:0043226  GO:0031967  GO:0031965   \n263755  GO:0005737  GO:0005829  GO:0005575  GO:0005622  GO:0110165   \n263756  GO:0005622  GO:0031981  GO:0043229  GO:0043226  GO:0110165   \n263757  GO:0005622  GO:0031981  GO:0043226  GO:0005730  GO:0043233   \n263758  GO:0005622  GO:0031981  GO:0043226  GO:0043233  GO:0005783   \n\n             term6       term7  ...      term34      term35      term36  \\\n0       GO:1901292  GO:0044237  ...  GO:1901575  GO:0072526  GO:0046434   \n1       GO:0003824        None  ...        None        None        None   \n2       GO:0007059  GO:0098813  ...  GO:0010628  GO:0021700  GO:0007064   \n3       GO:0000795  GO:0000785  ...        None        None        None   \n4             None        None  ...        None        None        None   \n...            ...         ...  ...         ...         ...         ...   \n263754  GO:0031975  GO:0005575  ...        None        None        None   \n263755        None        None  ...        None        None        None   \n263756  GO:0043231  GO:0070013  ...        None        None        None   \n263757  GO:0005575  GO:0043231  ...        None        None        None   \n263758  GO:0005575  GO:0031974  ...        None        None        None   \n\n            term37      term38      term39      term40      term41  \\\n0       GO:0009166  GO:0072524  GO:0006195  GO:0009056  GO:0044238   \n1             None        None        None        None        None   \n2       GO:0048285  GO:0006996  GO:0000070  GO:0000278  GO:0030182   \n3             None        None        None        None        None   \n4             None        None        None        None        None   \n...            ...         ...         ...         ...         ...   \n263754        None        None        None        None        None   \n263755        None        None        None        None        None   \n263756        None        None        None        None        None   \n263757        None        None        None        None        None   \n263758        None        None        None        None        None   \n\n            term42      term43  \n0       GO:0006793  GO:0019674  \n1             None        None  \n2       GO:0032501  GO:0010468  \n3             None        None  \n4             None        None  \n...            ...         ...  \n263754        None        None  \n263755        None        None  \n263756        None        None  \n263757        None        None  \n263758        None        None  \n\n[263759 rows x 46 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>sequence</th>\n      <th>aspect</th>\n      <th>term1</th>\n      <th>term2</th>\n      <th>term3</th>\n      <th>term4</th>\n      <th>term5</th>\n      <th>term6</th>\n      <th>term7</th>\n      <th>...</th>\n      <th>term34</th>\n      <th>term35</th>\n      <th>term36</th>\n      <th>term37</th>\n      <th>term38</th>\n      <th>term39</th>\n      <th>term40</th>\n      <th>term41</th>\n      <th>term42</th>\n      <th>term43</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A0A009IHW8</td>\n      <td>M S L E Q K K G A D I I S K I L Q I Q N S I G ...</td>\n      <td>BPO</td>\n      <td>GO:0008152</td>\n      <td>GO:0034655</td>\n      <td>GO:0072523</td>\n      <td>GO:0044270</td>\n      <td>GO:0006753</td>\n      <td>GO:1901292</td>\n      <td>GO:0044237</td>\n      <td>...</td>\n      <td>GO:1901575</td>\n      <td>GO:0072526</td>\n      <td>GO:0046434</td>\n      <td>GO:0009166</td>\n      <td>GO:0072524</td>\n      <td>GO:0006195</td>\n      <td>GO:0009056</td>\n      <td>GO:0044238</td>\n      <td>GO:0006793</td>\n      <td>GO:0019674</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A0A009IHW8</td>\n      <td>M S L E Q K K G A D I I S K I L Q I Q N S I G ...</td>\n      <td>MFO</td>\n      <td>GO:0003674</td>\n      <td>GO:0003953</td>\n      <td>GO:0016787</td>\n      <td>GO:0016799</td>\n      <td>GO:0016798</td>\n      <td>GO:0003824</td>\n      <td>None</td>\n      <td>...</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A0A021WW32</td>\n      <td>M F Y E H I I L A K K G P L A R I W L A A H W ...</td>\n      <td>BPO</td>\n      <td>GO:0048869</td>\n      <td>GO:0048856</td>\n      <td>GO:0022008</td>\n      <td>GO:0065007</td>\n      <td>GO:0007275</td>\n      <td>GO:0007059</td>\n      <td>GO:0098813</td>\n      <td>...</td>\n      <td>GO:0010628</td>\n      <td>GO:0021700</td>\n      <td>GO:0007064</td>\n      <td>GO:0048285</td>\n      <td>GO:0006996</td>\n      <td>GO:0000070</td>\n      <td>GO:0000278</td>\n      <td>GO:0030182</td>\n      <td>GO:0032501</td>\n      <td>GO:0010468</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A0A021WW32</td>\n      <td>M F Y E H I I L A K K G P L A R I W L A A H W ...</td>\n      <td>CCO</td>\n      <td>GO:0099086</td>\n      <td>GO:0000228</td>\n      <td>GO:0005622</td>\n      <td>GO:0043226</td>\n      <td>GO:0000792</td>\n      <td>GO:0000795</td>\n      <td>GO:0000785</td>\n      <td>...</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A0A021WZA4</td>\n      <td>M K Y I N C T Q P A I D D F P R D L F S E A Q ...</td>\n      <td>CCO</td>\n      <td>GO:0071944</td>\n      <td>GO:0005575</td>\n      <td>GO:0110165</td>\n      <td>GO:0016020</td>\n      <td>GO:0005886</td>\n      <td>None</td>\n      <td>None</td>\n      <td>...</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>263754</th>\n      <td>X6RKS3</td>\n      <td>M A N L G C W M L V L F V A T W S D L G L C K ...</td>\n      <td>CCO</td>\n      <td>GO:0005622</td>\n      <td>GO:0031090</td>\n      <td>GO:0043226</td>\n      <td>GO:0031967</td>\n      <td>GO:0031965</td>\n      <td>GO:0031975</td>\n      <td>GO:0005575</td>\n      <td>...</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>263755</th>\n      <td>X6RLN4</td>\n      <td>E V K G L F K S E N C P K V I S C E F A H N S ...</td>\n      <td>CCO</td>\n      <td>GO:0005737</td>\n      <td>GO:0005829</td>\n      <td>GO:0005575</td>\n      <td>GO:0005622</td>\n      <td>GO:0110165</td>\n      <td>None</td>\n      <td>None</td>\n      <td>...</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>263756</th>\n      <td>X6RLP6</td>\n      <td>M A A P E Q P L A I S R G C T S S S S L S P P ...</td>\n      <td>CCO</td>\n      <td>GO:0005622</td>\n      <td>GO:0031981</td>\n      <td>GO:0043229</td>\n      <td>GO:0043226</td>\n      <td>GO:0110165</td>\n      <td>GO:0043231</td>\n      <td>GO:0070013</td>\n      <td>...</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>263757</th>\n      <td>X6RLR1</td>\n      <td>M A G L T D L Q R L Q A R V E E L E R W V Y G ...</td>\n      <td>CCO</td>\n      <td>GO:0005622</td>\n      <td>GO:0031981</td>\n      <td>GO:0043226</td>\n      <td>GO:0005730</td>\n      <td>GO:0043233</td>\n      <td>GO:0005575</td>\n      <td>GO:0043231</td>\n      <td>...</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>263758</th>\n      <td>X6RM59</td>\n      <td>M D R A A V A R V G A V A S A S V C A L V A G ...</td>\n      <td>CCO</td>\n      <td>GO:0005622</td>\n      <td>GO:0031981</td>\n      <td>GO:0043226</td>\n      <td>GO:0043233</td>\n      <td>GO:0005783</td>\n      <td>GO:0005575</td>\n      <td>GO:0031974</td>\n      <td>...</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n  </tbody>\n</table>\n<p>263759 rows × 46 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"merged_df.columns","metadata":{"execution":{"iopub.status.busy":"2023-06-29T04:54:39.899088Z","iopub.execute_input":"2023-06-29T04:54:39.899706Z","iopub.status.idle":"2023-06-29T04:54:39.909676Z","shell.execute_reply.started":"2023-06-29T04:54:39.899643Z","shell.execute_reply":"2023-06-29T04:54:39.908299Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Index(['id', 'sequence', 'aspect', 'term1', 'term2', 'term3', 'term4', 'term5',\n       'term6', 'term7', 'term8', 'term9', 'term10', 'term11', 'term12',\n       'term13', 'term14', 'term15', 'term16', 'term17', 'term18', 'term19',\n       'term20', 'term21', 'term22', 'term23', 'term24', 'term25', 'term26',\n       'term27', 'term28', 'term29', 'term30', 'term31', 'term32', 'term33',\n       'term34', 'term35', 'term36', 'term37', 'term38', 'term39', 'term40',\n       'term41', 'term42', 'term43'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"merged_df2 = pd.merge(merged_df, taxonomy_df, on='id')\nnew_columns = ['id', 'taxonomyID', 'aspect', 'sequence', 'term1', 'term2', 'term3', 'term4', 'term5', 'term6', 'term7', 'term8', 'term9', 'term10', 'term11', 'term12', 'term13', 'term14', 'term15', 'term16', 'term17', 'term18', 'term19', 'term20', 'term21', 'term22', 'term23', 'term24', 'term25', 'term26', 'term27', 'term28', 'term29', 'term30', 'term31', 'term32', 'term33', 'term34', 'term35', 'term36', 'term37', 'term38', 'term39', 'term40', 'term41', 'term42', 'term43', 'term44', 'term45', 'term46', 'term47', 'term48', 'term49']\nmerged_df2 = merged_df2.reindex(columns=new_columns)\nmerged_df2","metadata":{"execution":{"iopub.status.busy":"2023-06-29T04:55:15.383352Z","iopub.execute_input":"2023-06-29T04:55:15.383778Z","iopub.status.idle":"2023-06-29T04:55:18.010966Z","shell.execute_reply.started":"2023-06-29T04:55:15.383745Z","shell.execute_reply":"2023-06-29T04:55:18.009989Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                id  taxonomyID aspect  \\\n0       A0A009IHW8     1310613    BPO   \n1       A0A009IHW8     1310613    MFO   \n2       A0A021WW32        7227    BPO   \n3       A0A021WW32        7227    CCO   \n4       A0A021WZA4        7227    CCO   \n...            ...         ...    ...   \n263754      X6RKS3        9606    CCO   \n263755      X6RLN4        9606    CCO   \n263756      X6RLP6        9606    CCO   \n263757      X6RLR1        9606    CCO   \n263758      X6RM59        9606    CCO   \n\n                                                 sequence       term1  \\\n0       M S L E Q K K G A D I I S K I L Q I Q N S I G ...  GO:0008152   \n1       M S L E Q K K G A D I I S K I L Q I Q N S I G ...  GO:0003674   \n2       M F Y E H I I L A K K G P L A R I W L A A H W ...  GO:0048869   \n3       M F Y E H I I L A K K G P L A R I W L A A H W ...  GO:0099086   \n4       M K Y I N C T Q P A I D D F P R D L F S E A Q ...  GO:0071944   \n...                                                   ...         ...   \n263754  M A N L G C W M L V L F V A T W S D L G L C K ...  GO:0005622   \n263755  E V K G L F K S E N C P K V I S C E F A H N S ...  GO:0005737   \n263756  M A A P E Q P L A I S R G C T S S S S L S P P ...  GO:0005622   \n263757  M A G L T D L Q R L Q A R V E E L E R W V Y G ...  GO:0005622   \n263758  M D R A A V A R V G A V A S A S V C A L V A G ...  GO:0005622   \n\n             term2       term3       term4       term5       term6  ...  \\\n0       GO:0034655  GO:0072523  GO:0044270  GO:0006753  GO:1901292  ...   \n1       GO:0003953  GO:0016787  GO:0016799  GO:0016798  GO:0003824  ...   \n2       GO:0048856  GO:0022008  GO:0065007  GO:0007275  GO:0007059  ...   \n3       GO:0000228  GO:0005622  GO:0043226  GO:0000792  GO:0000795  ...   \n4       GO:0005575  GO:0110165  GO:0016020  GO:0005886        None  ...   \n...            ...         ...         ...         ...         ...  ...   \n263754  GO:0031090  GO:0043226  GO:0031967  GO:0031965  GO:0031975  ...   \n263755  GO:0005829  GO:0005575  GO:0005622  GO:0110165        None  ...   \n263756  GO:0031981  GO:0043229  GO:0043226  GO:0110165  GO:0043231  ...   \n263757  GO:0031981  GO:0043226  GO:0005730  GO:0043233  GO:0005575  ...   \n263758  GO:0031981  GO:0043226  GO:0043233  GO:0005783  GO:0005575  ...   \n\n            term40      term41      term42      term43 term44 term45 term46  \\\n0       GO:0009056  GO:0044238  GO:0006793  GO:0019674    NaN    NaN    NaN   \n1             None        None        None        None    NaN    NaN    NaN   \n2       GO:0000278  GO:0030182  GO:0032501  GO:0010468    NaN    NaN    NaN   \n3             None        None        None        None    NaN    NaN    NaN   \n4             None        None        None        None    NaN    NaN    NaN   \n...            ...         ...         ...         ...    ...    ...    ...   \n263754        None        None        None        None    NaN    NaN    NaN   \n263755        None        None        None        None    NaN    NaN    NaN   \n263756        None        None        None        None    NaN    NaN    NaN   \n263757        None        None        None        None    NaN    NaN    NaN   \n263758        None        None        None        None    NaN    NaN    NaN   \n\n       term47 term48 term49  \n0         NaN    NaN    NaN  \n1         NaN    NaN    NaN  \n2         NaN    NaN    NaN  \n3         NaN    NaN    NaN  \n4         NaN    NaN    NaN  \n...       ...    ...    ...  \n263754    NaN    NaN    NaN  \n263755    NaN    NaN    NaN  \n263756    NaN    NaN    NaN  \n263757    NaN    NaN    NaN  \n263758    NaN    NaN    NaN  \n\n[263759 rows x 53 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>taxonomyID</th>\n      <th>aspect</th>\n      <th>sequence</th>\n      <th>term1</th>\n      <th>term2</th>\n      <th>term3</th>\n      <th>term4</th>\n      <th>term5</th>\n      <th>term6</th>\n      <th>...</th>\n      <th>term40</th>\n      <th>term41</th>\n      <th>term42</th>\n      <th>term43</th>\n      <th>term44</th>\n      <th>term45</th>\n      <th>term46</th>\n      <th>term47</th>\n      <th>term48</th>\n      <th>term49</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A0A009IHW8</td>\n      <td>1310613</td>\n      <td>BPO</td>\n      <td>M S L E Q K K G A D I I S K I L Q I Q N S I G ...</td>\n      <td>GO:0008152</td>\n      <td>GO:0034655</td>\n      <td>GO:0072523</td>\n      <td>GO:0044270</td>\n      <td>GO:0006753</td>\n      <td>GO:1901292</td>\n      <td>...</td>\n      <td>GO:0009056</td>\n      <td>GO:0044238</td>\n      <td>GO:0006793</td>\n      <td>GO:0019674</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A0A009IHW8</td>\n      <td>1310613</td>\n      <td>MFO</td>\n      <td>M S L E Q K K G A D I I S K I L Q I Q N S I G ...</td>\n      <td>GO:0003674</td>\n      <td>GO:0003953</td>\n      <td>GO:0016787</td>\n      <td>GO:0016799</td>\n      <td>GO:0016798</td>\n      <td>GO:0003824</td>\n      <td>...</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A0A021WW32</td>\n      <td>7227</td>\n      <td>BPO</td>\n      <td>M F Y E H I I L A K K G P L A R I W L A A H W ...</td>\n      <td>GO:0048869</td>\n      <td>GO:0048856</td>\n      <td>GO:0022008</td>\n      <td>GO:0065007</td>\n      <td>GO:0007275</td>\n      <td>GO:0007059</td>\n      <td>...</td>\n      <td>GO:0000278</td>\n      <td>GO:0030182</td>\n      <td>GO:0032501</td>\n      <td>GO:0010468</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A0A021WW32</td>\n      <td>7227</td>\n      <td>CCO</td>\n      <td>M F Y E H I I L A K K G P L A R I W L A A H W ...</td>\n      <td>GO:0099086</td>\n      <td>GO:0000228</td>\n      <td>GO:0005622</td>\n      <td>GO:0043226</td>\n      <td>GO:0000792</td>\n      <td>GO:0000795</td>\n      <td>...</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A0A021WZA4</td>\n      <td>7227</td>\n      <td>CCO</td>\n      <td>M K Y I N C T Q P A I D D F P R D L F S E A Q ...</td>\n      <td>GO:0071944</td>\n      <td>GO:0005575</td>\n      <td>GO:0110165</td>\n      <td>GO:0016020</td>\n      <td>GO:0005886</td>\n      <td>None</td>\n      <td>...</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>263754</th>\n      <td>X6RKS3</td>\n      <td>9606</td>\n      <td>CCO</td>\n      <td>M A N L G C W M L V L F V A T W S D L G L C K ...</td>\n      <td>GO:0005622</td>\n      <td>GO:0031090</td>\n      <td>GO:0043226</td>\n      <td>GO:0031967</td>\n      <td>GO:0031965</td>\n      <td>GO:0031975</td>\n      <td>...</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>263755</th>\n      <td>X6RLN4</td>\n      <td>9606</td>\n      <td>CCO</td>\n      <td>E V K G L F K S E N C P K V I S C E F A H N S ...</td>\n      <td>GO:0005737</td>\n      <td>GO:0005829</td>\n      <td>GO:0005575</td>\n      <td>GO:0005622</td>\n      <td>GO:0110165</td>\n      <td>None</td>\n      <td>...</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>263756</th>\n      <td>X6RLP6</td>\n      <td>9606</td>\n      <td>CCO</td>\n      <td>M A A P E Q P L A I S R G C T S S S S L S P P ...</td>\n      <td>GO:0005622</td>\n      <td>GO:0031981</td>\n      <td>GO:0043229</td>\n      <td>GO:0043226</td>\n      <td>GO:0110165</td>\n      <td>GO:0043231</td>\n      <td>...</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>263757</th>\n      <td>X6RLR1</td>\n      <td>9606</td>\n      <td>CCO</td>\n      <td>M A G L T D L Q R L Q A R V E E L E R W V Y G ...</td>\n      <td>GO:0005622</td>\n      <td>GO:0031981</td>\n      <td>GO:0043226</td>\n      <td>GO:0005730</td>\n      <td>GO:0043233</td>\n      <td>GO:0005575</td>\n      <td>...</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>263758</th>\n      <td>X6RM59</td>\n      <td>9606</td>\n      <td>CCO</td>\n      <td>M D R A A V A R V G A V A S A S V C A L V A G ...</td>\n      <td>GO:0005622</td>\n      <td>GO:0031981</td>\n      <td>GO:0043226</td>\n      <td>GO:0043233</td>\n      <td>GO:0005783</td>\n      <td>GO:0005575</td>\n      <td>...</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>263759 rows × 53 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"merged_df2.to_csv('id_taxid_aspect_seq_term.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T04:55:46.126876Z","iopub.execute_input":"2023-06-29T04:55:46.127330Z","iopub.status.idle":"2023-06-29T04:56:07.433560Z","shell.execute_reply.started":"2023-06-29T04:55:46.127293Z","shell.execute_reply":"2023-06-29T04:56:07.432321Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndf_max = pd.read_csv('/kaggle/input/2022-06-22/id_seq_term_tax.csv')\ndf_test = df_max[df_max['term1'] == 'GO:0008152'].drop(columns = ['term2', 'term3', 'term4', 'term5', 'term6', 'term7', 'term8', 'term9', 'term10', 'term11', 'term12', 'term13', 'term14', 'term15', 'term16', 'term17', 'term18', 'term19', 'term20', 'term21', 'term22', 'term23', 'term24', 'term25', 'term26', 'term27', 'term28', 'term29', 'term30', 'term31', 'term32', 'term33', 'term34', 'term35', 'term36', 'term37', 'term38', 'term39', 'term40', 'term41', 'term42', 'term43', 'term44', 'term45', 'term46', 'term47', 'term48', 'term49'])\ndf_test","metadata":{"execution":{"iopub.status.busy":"2023-06-28T08:09:54.877674Z","iopub.execute_input":"2023-06-28T08:09:54.878091Z","iopub.status.idle":"2023-06-28T08:10:01.408876Z","shell.execute_reply.started":"2023-06-28T08:09:54.878060Z","shell.execute_reply":"2023-06-28T08:10:01.407467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk import FreqDist\nfrom nltk.tokenize import word_tokenize\nfrom nltk.util import ngrams\n\n#아미노산 단어 갯수 측정 함수\ndef find_common(amino_acid_sequences):\n    # 아미노산 배열을 단어로 토큰화\n    tokens = [word_tokenize(seq) for seq in amino_acid_sequences]\n\n    # 모든 아미노산 배열의 단어들을 하나의 리스트로 통합\n    all_words = [word for seq in tokens for word in seq]\n\n    # 단어들의 빈도수 계산\n    freq_dist = FreqDist(all_words)\n\n    # 가장 빈도가 높은 상위 단어들을 반환\n    common_patterns = freq_dist.most_common(10)\n\n    return common_patterns\n\n#아미노산 패턴 확인 함수\ndef find_common_patterns(amino_acid_sequences, min_n, max_n, min_frequency):\n    # 아미노산 배열을 단어로 토큰화\n    tokens = [word_tokenize(seq) for seq in amino_acid_sequences]\n\n    # 모든 아미노산 배열의 단어들을 하나의 리스트로 통합\n    all_words = [word for seq in tokens for word in seq]\n\n    common_patterns = []\n    for n in range(min_n, max_n + 1):\n        # n-gram 생성\n        ngram_list = list(ngrams(all_words, n))\n\n        # n-gram 빈도수 계산\n        freq_dist = FreqDist(ngram_list)\n\n        # 가장 빈도가 높은 상위 n-gram들 중 발견 횟수가 min_frequency 이상인 것들을 추가\n        common_patterns.extend([(pattern, count) for pattern, count in freq_dist.most_common(10) if count >= min_frequency])\n\n    return common_patterns","metadata":{"execution":{"iopub.status.busy":"2023-06-27T10:26:25.063578Z","iopub.execute_input":"2023-06-27T10:26:25.064413Z","iopub.status.idle":"2023-06-27T10:26:26.552115Z","shell.execute_reply.started":"2023-06-27T10:26:25.064379Z","shell.execute_reply":"2023-06-27T10:26:26.550810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"amino_acid_sequences = df_test['sequence']","metadata":{"execution":{"iopub.status.busy":"2023-06-27T10:26:26.553511Z","iopub.execute_input":"2023-06-27T10:26:26.553864Z","iopub.status.idle":"2023-06-27T10:26:26.560355Z","shell.execute_reply.started":"2023-06-27T10:26:26.553835Z","shell.execute_reply":"2023-06-27T10:26:26.559124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2개 이상 이어진 패턴 찾기 (n-gram의 최소값: 2, 최대값: 5)\ncommon_patterns = find_common_patterns(amino_acid_sequences, 2, 5, 2000)\n#min_frequency를 2000으로 설정 = 발견 횟수가 min_frequency 이상인 패턴들만 반환\n#2는 n-gram의 크기를 나타내는 매개변수입니다. n-gram은 연속된 단어의 시퀀스로 구성된 패턴을 의미합니다. \n#위 코드의 경우 2로 설정하면 바이그램(bigram)을 생성하게 됩니다.\n\n# 결과 출력\nif common_patterns:\n    print(\"Common patterns found:\")\n    for pattern, count in common_patterns:\n        print(f\"Pattern: {pattern}, Count: {count}\")\nelse:\n    print(\"No common patterns found.\")","metadata":{"execution":{"iopub.status.busy":"2023-06-27T10:26:26.563539Z","iopub.execute_input":"2023-06-27T10:26:26.563977Z","iopub.status.idle":"2023-06-27T10:28:51.578461Z","shell.execute_reply.started":"2023-06-27T10:26:26.563938Z","shell.execute_reply":"2023-06-27T10:28:51.577325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"amino_acid_sequences.info()","metadata":{"execution":{"iopub.status.busy":"2023-06-27T10:28:51.580032Z","iopub.execute_input":"2023-06-27T10:28:51.581181Z","iopub.status.idle":"2023-06-27T10:28:51.606876Z","shell.execute_reply.started":"2023-06-27T10:28:51.581141Z","shell.execute_reply":"2023-06-27T10:28:51.605739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(amino_acid_sequences)","metadata":{"execution":{"iopub.status.busy":"2023-06-27T10:28:51.608419Z","iopub.execute_input":"2023-06-27T10:28:51.608873Z","iopub.status.idle":"2023-06-27T10:28:51.617150Z","shell.execute_reply.started":"2023-06-27T10:28:51.608831Z","shell.execute_reply":"2023-06-27T10:28:51.615838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndf_max = pd.read_csv('/kaggle/input/2022-06-22/id_seq_term_tax.csv')\n\n# term에 'x'가 들어간 얘들 모두 찾는 코드\ndef find_seq(x):\n    term_columns = ['term1', 'term2', 'term3', 'term4', 'term5', 'term6', 'term7', 'term8', 'term9', 'term10', 'term11', 'term12', 'term13', 'term14', 'term15', 'term16', 'term17', 'term18', 'term19', 'term20', 'term21', 'term22', 'term23', 'term24', 'term25', 'term26', 'term27', 'term28', 'term29', 'term30', 'term31', 'term32', 'term33', 'term34', 'term35', 'term36', 'term37', 'term38', 'term39', 'term40', 'term41', 'term42', 'term43', 'term44', 'term45', 'term46', 'term47', 'term48', 'term49']\n    df_find = pd.DataFrame()\n    for i in term_columns:\n        df_find = pd.concat([df_find, df_max[df_max[i] == x]])    \n    df_find.drop(columns = term_columns, axis=1, inplace=True)\n    return df_find\n\ndef find_same_seq(x):\n    amino_acid_sequences = x['sequence']\n    for i in range(len(amino_acid_sequences)):\n        if i == 0:\n            string1 = amino_acid_sequences.values[i]\n            string2 = amino_acid_sequences.values[i+1]\n            output = ''.join([char1 if char1 == char2 else '-' for char1, char2 in zip(string1, string2)])\n    \n        elif i > 0 | i+1 < k:\n            tring1 = output\n            string2 = amino_acid_sequences.values[i+1]\n            output = ''.join([char1 if char1 == char2 else '-' for char1, char2 in zip(string1, string2)])\n    return output\n\nresult = find_seq('GO:0008152')\nsame_seq= find_same_seq(result)\nprint(same_seq)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T08:11:35.418534Z","iopub.execute_input":"2023-06-28T08:11:35.418938Z","iopub.status.idle":"2023-06-28T08:11:41.435012Z","shell.execute_reply.started":"2023-06-28T08:11:35.418907Z","shell.execute_reply":"2023-06-28T08:11:41.433845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. GO:0005575    92912\n2. GO:0008150    92210\n3. GO:0110165    91286\n4. GO:0003674    78637\n5. GO:0005622    70785\n6. GO:0009987    61293\n7. GO:0043226    60883\n8. GO:0043229    58315\n9. GO:0005488    57380\n10. GO:0043227    55452","metadata":{}},{"cell_type":"code","source":"result = find_seq('GO:0005575')\nsame_seq= find_same_seq(result)\nprint(same_seq)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T08:13:57.773813Z","iopub.execute_input":"2023-06-28T08:13:57.774215Z","iopub.status.idle":"2023-06-28T08:14:01.641523Z","shell.execute_reply.started":"2023-06-28T08:13:57.774180Z","shell.execute_reply":"2023-06-28T08:14:01.640330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result2 = find_seq('GO:0008150')\nsame_seq2= find_same_seq(result2)\nprint(same_seq2)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T08:15:14.484022Z","iopub.execute_input":"2023-06-28T08:15:14.484447Z","iopub.status.idle":"2023-06-28T08:15:18.312361Z","shell.execute_reply.started":"2023-06-28T08:15:14.484414Z","shell.execute_reply":"2023-06-28T08:15:18.311264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result3 = find_seq('GO:0110165')\nsame_seq3= find_same_seq(result3)\nprint(same_seq3)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T08:16:05.429245Z","iopub.execute_input":"2023-06-28T08:16:05.429581Z","iopub.status.idle":"2023-06-28T08:16:08.287743Z","shell.execute_reply.started":"2023-06-28T08:16:05.429551Z","shell.execute_reply":"2023-06-28T08:16:08.286407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result4 = find_seq('GO:0003674')\nsame_seq4= find_same_seq(result4)\nprint(same_seq4)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T08:16:03.261154Z","iopub.execute_input":"2023-06-28T08:16:03.262417Z","iopub.status.idle":"2023-06-28T08:16:05.427320Z","shell.execute_reply.started":"2023-06-28T08:16:03.262339Z","shell.execute_reply":"2023-06-28T08:16:05.426107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"위는 직접 실행 해본 코드","metadata":{}},{"cell_type":"code","source":"#차라리 term이름만 넣으면 공통 amino acide 가 나오도록 def만 만들까?","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"!단백질 시퀀스 정보를 벡터로 변환하는 방법(임베딩)에 대한 소개와 연구 동향\n에서 소개된 embeding 방식\nhttps://github.com/nadavbra/protein_bert/tree/master/proteinbert\nhttps://github.com/mikejhuang/PhageProtVec","metadata":{}},{"cell_type":"markdown","source":"ProtBERT Vector Embeddings for Protein Sequences","metadata":{}},{"cell_type":"code","source":"MAIN_DIR = \"/kaggle/input/cafa-5-protein-function-prediction\"\n\n# UTILITARIES\nimport numpy as np\nfrom tqdm import tqdm\nimport time\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\n# TORCH MODULES FOR METRICS COMPUTATION :\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch import nn\nfrom torch.utils.data import random_split\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torchmetrics.classification import MultilabelF1Score\nfrom torchmetrics.classification import MultilabelAccuracy\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.loggers import WandbLogger\n\n# WANDB FOR LIGHTNING :\nimport wandb\n\n# FILES VISUALIZATION\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2023-06-28T08:29:41.884342Z","iopub.execute_input":"2023-06-28T08:29:41.885713Z","iopub.status.idle":"2023-06-28T08:29:43.650591Z","shell.execute_reply.started":"2023-06-28T08:29:41.885662Z","shell.execute_reply":"2023-06-28T08:29:43.649434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class config:\n    train_sequences_path = MAIN_DIR  + \"/Train/train_sequences.fasta\"\n    train_labels_path = MAIN_DIR + \"/Train/train_terms.tsv\"\n    test_sequences_path = MAIN_DIR + \"/Test (Targets)/testsuperset.fasta\"\n    \n    num_labels = 500\n    n_epochs = 5\n    batch_size = 128\n    lr = 0.001\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2023-06-28T08:29:46.507955Z","iopub.execute_input":"2023-06-28T08:29:46.508572Z","iopub.status.idle":"2023-06-28T08:29:46.516305Z","shell.execute_reply.started":"2023-06-28T08:29:46.508527Z","shell.execute_reply":"2023-06-28T08:29:46.514158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Load ProtBERT Model...\")\n# PROT BERT LOADING :\nfrom Bio import SeqIO\nfrom transformers import BertModel, BertTokenizer\ntokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\nmodel = BertModel.from_pretrained(\"Rostlab/prot_bert\").to(config.device)\n\ndef get_bert_embedding(\n    sequence : str,\n    len_seq_limit : int\n):\n    '''\n    Function to collect last hidden state embedding vector from pre-trained ProtBERT Model\n\n    INPUTS:\n    - sequence (str) : protein sequence (ex : AAABBB) from fasta file\n    - len_seq_limit (int) : maximum sequence lenght (i.e nb of letters) for truncation\n\n    OUTPUTS:\n    - output_hidden : last hidden state embedding vector for input sequence of length 1024\n    '''\n    sequence_w_spaces = ' '.join(list(sequence))\n    encoded_input = tokenizer(\n        sequence_w_spaces,\n        truncation=True,\n        max_length=len_seq_limit,\n        padding='max_length',\n        return_tensors='pt').to(config.device)\n    output = model(**encoded_input)\n    output_hidden = output['last_hidden_state'][:,0][0].detach().cpu().numpy()\n    assert len(output_hidden)==1024\n    return output_hidden\n\nseq_file = \"/kaggle/input/cafa-5-protein-function-prediction/Train/train_sequences.fasta\"\n\n### COLLECTING FOR TRAIN SAMPLES :\nprint(\"Loading train set ProtBERT Embeddings...\")\nfasta_train = SeqIO.parse(open(seq_file), \"fasta\")\nprint(\"Total Nb of Elements : \", len(list(fasta_train)))\nfasta_train = SeqIO.parse(open(seq_file), \"fasta\")\nids_list = []\nembed_vects_list = []\nt0 = time.time()\ncheckpoint = 0\nfor item in tqdm(fasta_train):\n    ids_list.append(item.id)\n    embed_vects_list.append(\n        get_bert_embedding(sequence = item.seq, len_seq_limit = 1200))\n    checkpoint+=1\n    if checkpoint>=100:\n        df_res = pd.DataFrame(data={\"id\" : ids_list, \"embed_vect\" : embed_vects_list})\n        np.save('/kaggle/working/train_ids.npy',np.array(ids_list))\n        np.save('/kaggle/working/train_embeddings.npy',np.array(embed_vects_list))\n        checkpoint=0\n\nnp.save('/kaggle/working/train_ids.npy',np.array(ids_list))\nnp.save('/kaggle/working/train_embeddings.npy',np.array(embed_vects_list))\nprint('Total Elapsed Time:',time.time()-t0)\n\n### COLLECTING FOR TEST SAMPLES :\nprint(\"Loading test set ProtBERT Embeddings...\")\nfasta_test = SeqIO.parse(config.test_sequences_path, \"fasta\")\nprint(\"Total Nb of Elements : \", len(list(fasta_test)))\nfasta_test = SeqIO.parse(config.test_sequences_path, \"fasta\")\nids_list = []\nembed_vects_list = []\nt0 = time.time()\ncheckpoint=0\nfor item in tqdm(fasta_test):\n    ids_list.append(item.id)\n    embed_vects_list.append(\n        get_bert_embedding(sequence = item.seq, len_seq_limit = 1200))\n    checkpoint+=1\n    if checkpoint>=100:\n        np.save('/kaggle/working/test_ids.npy',np.array(ids_list))\n        np.save('/kaggle/working/test_embeddings.npy',np.array(embed_vects_list))\n        checkpoint=0\n\nnp.save('/kaggle/working/test_ids.npy',np.array(ids_list))\nnp.save('/kaggle/working/test_embeddings.npy',np.array(embed_vects_list))\nprint('Total Elasped Time:',time.time()-t0)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-28T08:30:27.610283Z","iopub.execute_input":"2023-06-28T08:30:27.610679Z","iopub.status.idle":"2023-06-28T08:44:35.676597Z","shell.execute_reply.started":"2023-06-28T08:30:27.610648Z","shell.execute_reply":"2023-06-28T08:44:35.674586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"위 모델은 embeding test 중","metadata":{}},{"cell_type":"code","source":"from __future__ import print_function,division\n\nimport sys\nimport numpy as np\nimport h5py\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom src.alphabets import Uniprot21\nimport src.fasta as fasta\nimport src.models.sequence\n\n\ndef unstack_lstm(lstm):\n    device = next(iter(lstm.parameters())).device\n\n    in_size = lstm.input_size\n    hidden_dim = lstm.hidden_size\n    layers = []\n    for i in range(lstm.num_layers):\n        layer = nn.LSTM(in_size, hidden_dim, batch_first=True, bidirectional=True)\n        layer.to(device)\n\n        attributes = ['weight_ih_l', 'weight_hh_l', 'bias_ih_l', 'bias_hh_l']\n        for attr in attributes:\n            dest = attr + '0'\n            src = attr + str(i)\n            getattr(layer, dest).data[:] = getattr(lstm, src)\n            #setattr(layer, dest, getattr(lstm, src))\n            \n            dest = attr + '0_reverse'\n            src = attr + str(i) + '_reverse'\n            getattr(layer, dest).data[:] = getattr(lstm, src)\n            #setattr(layer, dest, getattr(lstm, src))\n        layer.flatten_parameters()\n        layers.append(layer)\n        in_size = 2*hidden_dim\n    return layers\n\ndef embed_stack(x, lm_embed, lstm_stack, proj, include_lm=True, final_only=False):\n    zs = []\n    \n    x_onehot = x.new(x.size(0),x.size(1), 21).float().zero_()\n    x_onehot.scatter_(2,x.unsqueeze(2),1)\n    zs.append(x_onehot)\n    \n    h = lm_embed(x)\n    if include_lm and not final_only:\n        zs.append(h)\n\n    if lstm_stack is not None:\n        for lstm in lstm_stack:\n            h,_ = lstm(h)\n            if not final_only:\n                zs.append(h)\n        h = proj(h.squeeze(0)).unsqueeze(0)\n        zs.append(h)\n\n    z = torch.cat(zs, 2)\n    return z\n\n\ndef embed_sequence(x, lm_embed, lstm_stack, proj, include_lm=True, final_only=False\n                  ,  pool='none', use_cuda=False):\n\n    if len(x) == 0:\n        return None\n\n    alphabet = Uniprot21()\n    x = x.upper()\n    # convert to alphabet index\n    x = alphabet.encode(x)\n    x = torch.from_numpy(x)\n    if use_cuda:\n        x = x.cuda()\n\n    # embed the sequence\n    with torch.no_grad():\n        x = x.long().unsqueeze(0)\n        z = embed_stack(x, lm_embed, lstm_stack, proj\n                       , include_lm=include_lm, final_only=final_only)\n        # pool if needed\n        z = z.squeeze(0)\n        if pool == 'sum':\n            z = z.sum(0)\n        elif pool == 'max':\n            z,_ = z.max(0)\n        elif pool == 'avg':\n            z = z.mean(0)\n        z = z.cpu().numpy()\n\n    return z\n\n\ndef load_model(path, use_cuda=False):\n    encoder = torch.load(path)\n    encoder.eval()\n\n    if use_cuda:\n        encoder.cuda()\n\n    if type(encoder) is src.models.sequence.BiLM:\n        # model is only the LM\n        return encoder.encode, None, None\n\n    encoder = encoder.embedding\n\n    lm_embed = encoder.embed\n    lstm_stack = unstack_lstm(encoder.rnn)\n    proj = encoder.proj\n\n    return lm_embed, lstm_stack, proj\n\n\ndef main():\n    import argparse\n    parser = argparse.ArgumentParser('Script for embedding fasta format sequences using a saved embedding model. Saves embeddings as HDF5 file.')\n\n    parser.add_argument('path', help='sequences to embed in fasta format')\n    parser.add_argument('-m', '--model', help='path to saved embedding model')\n    parser.add_argument('-o', '--output', help='path to HDF5 output file')\n    parser.add_argument('--lm-only', action='store_true', help='only return the language model hidden layers')\n    parser.add_argument('--no-lm', action='store_true', help='do not include LM hidden layers in embedding. by default, all hidden layers of all layers are concatenated and returned by this script.')\n    parser.add_argument('--proj-only', action='store_true', help='only return the final structure-learned embedding')\n    parser.add_argument('--pool', choices=['none', 'sum', 'max', 'avg'], default='none', help='apply some pooling operation over each sequence (default: none)')\n    parser.add_argument('-d', '--device', type=int, default=-2, help='compute device to use')\n\n    args = parser.parse_args()\n\n    path = args.path\n\n    # set the device\n    d = args.device\n    use_cuda = (d != -1) and torch.cuda.is_available()\n    if d >= 0:\n        torch.cuda.set_device(d)\n\n    # load the model\n    lm_embed, lstm_stack, proj = load_model(args.model, use_cuda=use_cuda)\n\n    # parse the sequences and embed them\n    # write them to hdf5 file\n    print('# writing:', args.output, file=sys.stderr)\n    h5 = h5py.File(args.output, 'w')\n\n    lm_only = args.lm_only\n    if lm_only:\n        lstm_stack = None\n        proj = None\n\n    no_lm = args.no_lm\n    include_lm = not no_lm\n    final_only = args.proj_only\n\n    pool = args.pool\n    print('# embedding with lm_only={}, no_lm={}, proj_only={}'.format(lm_only, no_lm, final_only), file=sys.stderr)\n    print('# pooling:', pool, file=sys.stderr)\n\n    count = 0\n    with open(path, 'rb') as f:\n        for name,sequence in fasta.parse_stream(f):\n            # use sequence name as HDF key\n            pid = name.decode('utf-8')\n            if len(sequence) == 0:\n                print('# WARNING: sequence', pid, 'has length=0. Skipping.', file=sys.stderr)\n                continue\n            # only do pids we haven't done already...\n            if pid not in h5:\n                z = embed_sequence(sequence, lm_embed, lstm_stack, proj\n                                  , include_lm=include_lm, final_only=final_only\n                                  , pool=pool, use_cuda=use_cuda)\n                # write as hdf5 dataset\n                h5.create_dataset(pid, data=z, compression='lzf')\n            count += 1\n            print('# {} sequences processed...'.format(count), file=sys.stderr, end='\\r')\n    print(' '*80, file=sys.stderr, end='\\r')\n    print('# Done!', file=sys.stderr)\n\n\n\nif __name__ == '__main__':\n    main()\n\n\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#seq_file에서 dataframe 형식으로 data 불러오기\nseq_file = \"/kaggle/input/cafa-5-protein-function-prediction/Test (Targets)/testsuperset.fasta\"","metadata":{"execution":{"iopub.status.busy":"2023-06-28T06:11:01.237141Z","iopub.execute_input":"2023-06-28T06:11:01.237578Z","iopub.status.idle":"2023-06-28T06:11:01.243419Z","shell.execute_reply.started":"2023-06-28T06:11:01.237546Z","shell.execute_reply":"2023-06-28T06:11:01.242064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport re\ndef read_fasta(file_path, columns) :\n    from Bio.SeqIO.FastaIO import SimpleFastaParser \n    with open(file_path) as fasta_file :  \n        records = [] # create empty list\n\n        for title, sequence in SimpleFastaParser(fasta_file):  #SimpleFastaParser Iterate over Fasta records as string tuples. For each record a tuple of two strings is returned: the FASTA title line (without the leading ‘>’ character),  and the sequence (with any whitespace removed). \n            record = []\n            title_splits=re.findall(r\"[\\w']+\", title) # Data cleaning is needed\n            record.append(title_splits[0])  #First values are ID (Append adds element to a list)\n            record.append(len(sequence)) #Second values are sequences lengths\n            sequence = \" \".join(sequence) #It converts into one line\n            record.append(sequence)#Third values are sequences\n            records.append(record)\n    return pd.DataFrame(records, columns = columns) #We have created a function that returns a dataframe\n\n#Now let's use this function by inserting in the first argument the file name (or file path if your working directory is different from where the fasta file is)        \n#And in the second one the names of columns\ndata = read_fasta(seq_file, columns=[\"id\",\"sequence_length\", \"sequence\"])\ndata\nvar_seq = data.loc[0,'sequence']","metadata":{"execution":{"iopub.status.busy":"2023-06-28T06:14:05.906709Z","iopub.execute_input":"2023-06-28T06:14:05.907141Z","iopub.status.idle":"2023-06-28T06:14:09.762732Z","shell.execute_reply.started":"2023-06-28T06:14:05.907106Z","shell.execute_reply":"2023-06-28T06:14:09.761680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#불러온 정보 쪼개기\nfrom Bio import SeqIO\n\n# Open the FASTA file\nfasta_sequences = SeqIO.parse(open(seq_file),'fasta')\nfor fasta in fasta_sequences:\n    print(fasta)\n    print(\"----------------------\")\n    print(\"Seqence ID:\", fasta.id)\n    print(\"----------------------\")\n    print(\"Seqence name:\", fasta.name)\n    print(\"----------------------\")\n    fasta.description = fasta.description.split(\" \")\n    #seqence 설명 된 부분 쪼개기. 특성을 얻고잫 하였으나 각 seq마다 위치가 달라서 가능할지 의문\n    #이걸 쪼개서 dataframe에 넣고 분류 기준으로 잡으면 좋을것 같다.\n    print(\"Seqence description:\", fasta.description[1:])\n    print(\"----------------------\")    \n    print(\"Seqence:\", repr(fasta.seq))\n    print(\"----------------------\")  \n    print(fasta.annotations)\n    break","metadata":{"execution":{"iopub.status.busy":"2023-06-28T06:11:07.159988Z","iopub.execute_input":"2023-06-28T06:11:07.160475Z","iopub.status.idle":"2023-06-28T06:11:07.198845Z","shell.execute_reply.started":"2023-06-28T06:11:07.160442Z","shell.execute_reply":"2023-06-28T06:11:07.197057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"var_seq","metadata":{"execution":{"iopub.status.busy":"2023-06-28T06:14:09.903715Z","iopub.execute_input":"2023-06-28T06:14:09.907293Z","iopub.status.idle":"2023-06-28T06:14:09.916166Z","shell.execute_reply.started":"2023-06-28T06:14:09.907241Z","shell.execute_reply":"2023-06-28T06:14:09.915201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ALL_AAS = 'ACDEFGHIKLMNPQRSTUVWXY'\nADDITIONAL_TOKENS = ['<OTHER>', '<START>', '<END>', '<PAD>']\n\n# Each sequence is added <START> and <END> tokens\nADDED_TOKENS_PER_SEQ = 2\n\nn_aas = len(ALL_AAS)\naa_to_token_index = {aa: i for i, aa in enumerate(ALL_AAS)}\nadditional_token_to_index = {token: i + n_aas for i, token in enumerate(ADDITIONAL_TOKENS)}\ntoken_to_index = {**aa_to_token_index, **additional_token_to_index}\nindex_to_token = {index: token for token, index in token_to_index.items()}\nn_tokens = len(token_to_index)\n\ndef tokenize_seq(seq):\n    other_token_index = additional_token_to_index['<OTHER>']\n    return [additional_token_to_index['<START>']] + [aa_to_token_index.get(aa, other_token_index) for aa in parse_seq(seq)] + \\\n            [additional_token_to_index['<END>']]\n            \ndef parse_seq(seq):\n    if isinstance(seq, str):\n        return seq\n    elif isinstance(seq, bytes):\n        return seq.decode('utf8')\n    else:\n        raise TypeError('Unexpected sequence type: %s' % type(seq))\n        \ntokenize_seq(var_seq)\n# parse_seq(var_seq)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T06:18:00.954829Z","iopub.execute_input":"2023-06-28T06:18:00.955325Z","iopub.status.idle":"2023-06-28T06:18:00.975080Z","shell.execute_reply.started":"2023-06-28T06:18:00.955284Z","shell.execute_reply":"2023-06-28T06:18:00.973724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://github.com/murhekar/aa-vec-embeddings/blob/master/scripts/models.py\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC6061698/\nhttps://www.kaggle.com/code/danofer/deep-protein-sequence-family-classification/notebook\nhttps://www.kaggle.com/code/basu369victor/attention-based-protein-structure-prediction","metadata":{}}]}