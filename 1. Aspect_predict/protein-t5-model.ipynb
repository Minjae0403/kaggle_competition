{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-18T11:26:37.434003Z","iopub.execute_input":"2023-07-18T11:26:37.434456Z","iopub.status.idle":"2023-07-18T11:26:37.500400Z","shell.execute_reply.started":"2023-07-18T11:26:37.434417Z","shell.execute_reply":"2023-07-18T11:26:37.499035Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/cafa-5-protein-function-prediction/sample_submission.tsv\n/kaggle/input/cafa-5-protein-function-prediction/IA.txt\n/kaggle/input/cafa-5-protein-function-prediction/Test (Targets)/testsuperset.fasta\n/kaggle/input/cafa-5-protein-function-prediction/Test (Targets)/testsuperset-taxon-list.tsv\n/kaggle/input/cafa-5-protein-function-prediction/Train/train_terms.tsv\n/kaggle/input/cafa-5-protein-function-prediction/Train/train_sequences.fasta\n/kaggle/input/cafa-5-protein-function-prediction/Train/train_taxonomy.tsv\n/kaggle/input/cafa-5-protein-function-prediction/Train/go-basic.obo\n/kaggle/input/cafa5-test/id_seq_aspect_tokenized.csv\n/kaggle/input/2022-06-22/test_aspect.npy\n/kaggle/input/2022-06-22/id_seq_aspect.csv\n/kaggle/input/2022-06-22/test_embeddings_vector.npy\n/kaggle/input/2022-06-22/train_embedding_vector.npy\n/kaggle/input/2022-06-22/train_aspect.npy\n/kaggle/input/2022-06-22/id_seq_term_tax.csv\n/kaggle/input/2022-06-22/id_taxid_aspect_seq_term.csv\n/kaggle/input/protbert-embeddings-for-cafa5/train_ids.npy\n/kaggle/input/protbert-embeddings-for-cafa5/train_embeddings.npy\n/kaggle/input/protbert-embeddings-for-cafa5/test_ids.npy\n/kaggle/input/protbert-embeddings-for-cafa5/test_embeddings.npy\n","output_type":"stream"}]},{"cell_type":"code","source":"import warnings\n\n# 경고 메시지를 무시하고 숨기거나\nwarnings.filterwarnings(action='ignore')","metadata":{"execution":{"iopub.status.busy":"2023-07-18T11:26:39.939092Z","iopub.execute_input":"2023-07-18T11:26:39.939607Z","iopub.status.idle":"2023-07-18T11:26:39.946594Z","shell.execute_reply.started":"2023-07-18T11:26:39.939563Z","shell.execute_reply":"2023-07-18T11:26:39.944786Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"**Fine-tuning Base**","metadata":{}},{"cell_type":"code","source":"!pip install evaluate\n!pip install deepspeed","metadata":{"execution":{"iopub.status.busy":"2023-07-18T11:27:19.782658Z","iopub.execute_input":"2023-07-18T11:27:19.783200Z","iopub.status.idle":"2023-07-18T11:27:35.834993Z","shell.execute_reply.started":"2023-07-18T11:27:19.783154Z","shell.execute_reply":"2023-07-18T11:27:35.833265Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.23.5)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.5.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.65.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.6.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (9.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\nInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.0\n","output_type":"stream"}]},{"cell_type":"code","source":"#import dependencies\nimport os.path\n# os.chdir(\"set a path here\")\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\nfrom torch.utils.data import DataLoader\n\nimport re\nimport numpy as np\nimport pandas as pd\nimport copy\n\nimport transformers, datasets\nfrom transformers.modeling_outputs import TokenClassifierOutput\nfrom transformers.models.t5.modeling_t5 import T5Config, T5PreTrainedModel, T5Stack\nfrom transformers.utils.model_parallel_utils import assert_device_map, get_device_map\nfrom transformers import T5EncoderModel, T5Tokenizer\nfrom transformers import TrainingArguments, Trainer, set_seed\nfrom transformers import DataCollatorForTokenClassification\n\nfrom evaluate import load\nfrom datasets import Dataset\n\nfrom tqdm import tqdm\nimport random\n\nfrom scipy import stats\nfrom sklearn.metrics import accuracy_score\n\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-07-18T11:27:35.838457Z","iopub.execute_input":"2023-07-18T11:27:35.839017Z","iopub.status.idle":"2023-07-18T11:27:38.181070Z","shell.execute_reply.started":"2023-07-18T11:27:35.838963Z","shell.execute_reply":"2023-07-18T11:27:38.180168Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Set environment variables to run Deepspeed from a notebook\nos.environ[\"MASTER_ADDR\"] = \"localhost\"\nos.environ[\"MASTER_PORT\"] = \"9993\"  # modify if RuntimeError: Address already in use\nos.environ[\"RANK\"] = \"0\"\nos.environ[\"LOCAL_RANK\"] = \"0\"\nos.environ[\"WORLD_SIZE\"] = \"1\"","metadata":{"execution":{"iopub.status.busy":"2023-07-18T11:27:38.182564Z","iopub.execute_input":"2023-07-18T11:27:38.183344Z","iopub.status.idle":"2023-07-18T11:27:38.191249Z","shell.execute_reply.started":"2023-07-18T11:27:38.183299Z","shell.execute_reply":"2023-07-18T11:27:38.190162Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(\"Torch version: \",torch.__version__)\nprint(\"Cuda version: \",torch.version.cuda)\nprint(\"Numpy version: \",np.__version__)\nprint(\"Pandas version: \",pd.__version__)\nprint(\"Transformers version: \",transformers.__version__)\nprint(\"Datasets version: \",datasets.__version__)","metadata":{"execution":{"iopub.status.busy":"2023-07-18T11:27:38.194617Z","iopub.execute_input":"2023-07-18T11:27:38.195281Z","iopub.status.idle":"2023-07-18T11:27:38.212398Z","shell.execute_reply.started":"2023-07-18T11:27:38.195233Z","shell.execute_reply":"2023-07-18T11:27:38.211042Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Torch version:  2.0.0+cpu\nCuda version:  None\nNumpy version:  1.23.5\nPandas version:  1.5.3\nTransformers version:  4.30.2\nDatasets version:  2.1.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Fine tuning Data","metadata":{}},{"cell_type":"code","source":"# For this example we import the secondary_structure dataset from https://github.com/J-SNACKKB/FLIP\n# For details, see publication here: https://openreview.net/forum?id=p2dMLEwL8tF\nimport requests\nimport zipfile\nfrom io import BytesIO\nfrom Bio import SeqIO\nimport tempfile\n\n# Download the zip file from GitHub\nurl = 'https://github.com/J-SNACKKB/FLIP/raw/main/splits/secondary_structure/splits.zip'\n\nresponse = requests.get(url)\nzip_file = zipfile.ZipFile(BytesIO(response.content))\n\n# Extract the fasta file to a temporary directory\n# Sequence File\nwith tempfile.TemporaryDirectory() as temp_dir:\n    zip_file.extract('splits/sequences.fasta', temp_dir)\n\n    # Load the fasta files\n    fasta_file = open(temp_dir + '/splits/sequences.fasta')\n    \n    # Load FASTA file using Biopython\n    sequences = []\n    for record in SeqIO.parse(fasta_file, \"fasta\"):\n        sequences.append([record.name, str(record.seq)])\n\n    # Create dataframe\n    df = pd.DataFrame(sequences, columns=[\"name\", \"sequence\"])\n\n# Mask File\nwith tempfile.TemporaryDirectory() as temp_dir:\n    zip_file.extract('splits/mask.fasta', temp_dir)\n\n    # Load the fasta files\n    fasta_file = open(temp_dir + '/splits/mask.fasta')\n    \n    # Load FASTA file using Biopython\n    sequences = []\n    for record in SeqIO.parse(fasta_file, \"fasta\"):\n        sequences.append([str(record.seq)])\n\n    # Add to dataframe\n    df = pd.concat([df, pd.DataFrame(sequences, columns=[\"mask\"])], axis=1) \n    \n# Label File\nwith tempfile.TemporaryDirectory() as temp_dir:\n    zip_file.extract('splits/sampled.fasta', temp_dir)\n\n    # Load the fasta files\n    fasta_file = open(temp_dir + '/splits/sampled.fasta')\n    \n    # Load FASTA file using Biopython\n    sequences = []\n    for record in SeqIO.parse(fasta_file, \"fasta\"):\n\n        sequences.append([str(record.seq), record.description])\n\n    # Add to dataframe\n    df = pd.concat([df, pd.DataFrame(sequences, columns=[ \"label\", \"dataset\"])], axis=1)  \n\n# Get data split information\ndf[\"validation\"]=df.dataset.str.split(\"=\").str[2]\n# str to bool\ndf['validation'] = df['validation'].apply(lambda x: x == 'True')\n\n# Extract data split information\ndf[\"dataset\"]=df.dataset.str.split(\"=\").str[1]\ndf[\"dataset\"]=df.dataset.str.split(\" \").str[0]\n\n# Preprocess mask and label to lists\n# C is class 0, E is class 1, H is class 2\ndf['label'] = df['label'].str.replace(\"C\",\"0\")\ndf['label'] = df['label'].str.replace(\"E\",\"1\")\ndf['label'] = df['label'].str.replace(\"H\",\"2\")\n\n# str to integer\ndf['label'] = df['label'].apply(lambda x: [int(i) for i in x])\ndf['mask'] = df['mask'].apply(lambda x: [int(i) for i in x])\n\n\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-07-18T11:27:38.214141Z","iopub.execute_input":"2023-07-18T11:27:38.214720Z","iopub.status.idle":"2023-07-18T11:27:41.923800Z","shell.execute_reply.started":"2023-07-18T11:27:38.214686Z","shell.execute_reply":"2023-07-18T11:27:41.922588Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"     name                                           sequence  \\\n0  1es5-A  VTKPTIAAVGGYAMNNGTGTTLYTKAADTRRSTGSTTKIMTAKVVL...   \n1  2a6h-E  MAEPGIDKLFGMVDSKYRLTVVVAKRAQQLLRHGFKNTVLEPEERP...   \n2  5b1a-P  MTHQTHAYHMVNPSPWPLTGALSALLMTSGLTMWFHFNSMTLLMIG...   \n3  5ehi-C  GTGSQGETLGEKWKKKLNQLSRKEFDLYKKSGITEVDRTEAKEGLK...   \n4  5egf-A  HHHHHHAVAKDSTESKSWEPFSLSPIKDPQALHAALCSKNVIPVTS...   \n\n                                                mask  \\\n0  [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n1  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n2  [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n3  [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                                               label dataset  validation  \n0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, ...   train       False  \n1  [0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, ...   train       False  \n2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   train       False  \n3  [0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, ...   train       False  \n4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   train       False  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>sequence</th>\n      <th>mask</th>\n      <th>label</th>\n      <th>dataset</th>\n      <th>validation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1es5-A</td>\n      <td>VTKPTIAAVGGYAMNNGTGTTLYTKAADTRRSTGSTTKIMTAKVVL...</td>\n      <td>[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>train</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2a6h-E</td>\n      <td>MAEPGIDKLFGMVDSKYRLTVVVAKRAQQLLRHGFKNTVLEPEERP...</td>\n      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, ...</td>\n      <td>train</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5b1a-P</td>\n      <td>MTHQTHAYHMVNPSPWPLTGALSALLMTSGLTMWFHFNSMTLLMIG...</td>\n      <td>[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>train</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5ehi-C</td>\n      <td>GTGSQGETLGEKWKKKLNQLSRKEFDLYKKSGITEVDRTEAKEGLK...</td>\n      <td>[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, ...</td>\n      <td>train</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5egf-A</td>\n      <td>HHHHHHAVAKDSTESKSWEPFSLSPIKDPQALHAALCSKNVIPVTS...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>train</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Seperate test and train data \nmy_test=df[df.dataset==\"test\"].reset_index(drop=True)\ndf=df[df.dataset==\"train\"]\n\n# Get train and validation data\nmy_train=df[df.validation!=True].reset_index(drop=True)\nmy_valid=df[df.validation==True].reset_index(drop=True)\n\n# Drop unneeded columns\nmy_train= my_train[[\"sequence\",\"label\",\"mask\"]]\nmy_valid= my_valid[[\"sequence\",\"label\",\"mask\"]]\nmy_test =  my_test[[\"sequence\",\"label\",\"mask\"]]\n\n# Set labels where mask == 0 to -100 (will be ignored by pytorch loss)\nmy_train['label'] = my_train.apply(lambda row: [-100 if m == 0 else l for l, m in zip(row['label'], row['mask'])], axis=1)\nmy_valid['label'] = my_valid.apply(lambda row: [-100 if m == 0 else l for l, m in zip(row['label'], row['mask'])], axis=1)\nmy_test['label'] = my_test.apply(lambda row: [-100 if m == 0 else l for l, m in zip(row['label'], row['mask'])], axis=1)\nmy_train.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-07-18T11:27:41.925017Z","iopub.execute_input":"2023-07-18T11:27:41.925382Z","iopub.status.idle":"2023-07-18T11:27:42.641524Z","shell.execute_reply.started":"2023-07-18T11:27:41.925353Z","shell.execute_reply":"2023-07-18T11:27:42.640321Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                            sequence  \\\n0  VTKPTIAAVGGYAMNNGTGTTLYTKAADTRRSTGSTTKIMTAKVVL...   \n1  MAEPGIDKLFGMVDSKYRLTVVVAKRAQQLLRHGFKNTVLEPEERP...   \n2  MTHQTHAYHMVNPSPWPLTGALSALLMTSGLTMWFHFNSMTLLMIG...   \n3  GTGSQGETLGEKWKKKLNQLSRKEFDLYKKSGITEVDRTEAKEGLK...   \n4  HHHHHHAVAKDSTESKSWEPFSLSPIKDPQALHAALCSKNVIPVTS...   \n\n                                               label  \\\n0  [-100, -100, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...   \n1  [-100, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, ...   \n2  [-100, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n3  [-100, -100, -100, -100, -100, -100, 0, 0, 2, ...   \n4  [-100, -100, -100, -100, -100, -100, -100, -10...   \n\n                                                mask  \n0  [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n1  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n2  [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n3  [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sequence</th>\n      <th>label</th>\n      <th>mask</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>VTKPTIAAVGGYAMNNGTGTTLYTKAADTRRSTGSTTKIMTAKVVL...</td>\n      <td>[-100, -100, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...</td>\n      <td>[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>MAEPGIDKLFGMVDSKYRLTVVVAKRAQQLLRHGFKNTVLEPEERP...</td>\n      <td>[-100, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, ...</td>\n      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MTHQTHAYHMVNPSPWPLTGALSALLMTSGLTMWFHFNSMTLLMIG...</td>\n      <td>[-100, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>GTGSQGETLGEKWKKKLNQLSRKEFDLYKKSGITEVDRTEAKEGLK...</td>\n      <td>[-100, -100, -100, -100, -100, -100, 0, 0, 2, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HHHHHHAVAKDSTESKSWEPFSLSPIKDPQALHAALCSKNVIPVTS...</td>\n      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Modifies an existing transformer and introduce the LoRA layers\n\nclass LoRAConfig:\n    def __init__(self):\n        self.lora_rank = 4\n        self.lora_init_scale = 0.01\n        self.lora_modules = \".*SelfAttention|.*EncDecAttention\"\n        self.lora_layers = \"q|k|v|o\"\n        self.trainable_param_names = \".*layer_norm.*|.*lora_[ab].*\"\n        self.lora_scaling_rank = 1\n        # lora_modules and lora_layers are speicified with regular expressions\n        # see https://www.w3schools.com/python/python_regex.asp for reference\n        \nclass LoRALinear(nn.Module):\n    def __init__(self, linear_layer, rank, scaling_rank, init_scale):\n        super().__init__()\n        self.in_features = linear_layer.in_features\n        self.out_features = linear_layer.out_features\n        self.rank = rank\n        self.scaling_rank = scaling_rank\n        self.weight = linear_layer.weight\n        self.bias = linear_layer.bias\n        if self.rank > 0:\n            self.lora_a = nn.Parameter(torch.randn(rank, linear_layer.in_features) * init_scale)\n            if init_scale < 0:\n                self.lora_b = nn.Parameter(torch.randn(linear_layer.out_features, rank) * init_scale)\n            else:\n                self.lora_b = nn.Parameter(torch.zeros(linear_layer.out_features, rank))\n        if self.scaling_rank:\n            self.multi_lora_a = nn.Parameter(\n                torch.ones(self.scaling_rank, linear_layer.in_features)\n                + torch.randn(self.scaling_rank, linear_layer.in_features) * init_scale\n            )\n            if init_scale < 0:\n                self.multi_lora_b = nn.Parameter(\n                    torch.ones(linear_layer.out_features, self.scaling_rank)\n                    + torch.randn(linear_layer.out_features, self.scaling_rank) * init_scale\n                )\n            else:\n                self.multi_lora_b = nn.Parameter(torch.ones(linear_layer.out_features, self.scaling_rank))\n\n    def forward(self, input):\n        if self.scaling_rank == 1 and self.rank == 0:\n            # parsimonious implementation for ia3 and lora scaling\n            if self.multi_lora_a.requires_grad:\n                hidden = F.linear((input * self.multi_lora_a.flatten()), self.weight, self.bias)\n            else:\n                hidden = F.linear(input, self.weight, self.bias)\n            if self.multi_lora_b.requires_grad:\n                hidden = hidden * self.multi_lora_b.flatten()\n            return hidden\n        else:\n            # general implementation for lora (adding and scaling)\n            weight = self.weight\n            if self.scaling_rank:\n                weight = weight * torch.matmul(self.multi_lora_b, self.multi_lora_a) / self.scaling_rank\n            if self.rank:\n                weight = weight + torch.matmul(self.lora_b, self.lora_a) / self.rank\n            return F.linear(input, weight, self.bias)\n\n    def extra_repr(self):\n        return \"in_features={}, out_features={}, bias={}, rank={}, scaling_rank={}\".format(\n            self.in_features, self.out_features, self.bias is not None, self.rank, self.scaling_rank\n        )\n\n\ndef modify_with_lora(transformer, config):\n    for m_name, module in dict(transformer.named_modules()).items():\n        if re.fullmatch(config.lora_modules, m_name):\n            for c_name, layer in dict(module.named_children()).items():\n                if re.fullmatch(config.lora_layers, c_name):\n                    assert isinstance(\n                        layer, nn.Linear\n                    ), f\"LoRA can only be applied to torch.nn.Linear, but {layer} is {type(layer)}.\"\n                    setattr(\n                        module,\n                        c_name,\n                        LoRALinear(layer, config.lora_rank, config.lora_scaling_rank, config.lora_init_scale),\n                    )\n    return transformer","metadata":{"execution":{"iopub.status.busy":"2023-07-18T11:27:42.642898Z","iopub.execute_input":"2023-07-18T11:27:42.643245Z","iopub.status.idle":"2023-07-18T11:27:42.666425Z","shell.execute_reply.started":"2023-07-18T11:27:42.643209Z","shell.execute_reply":"2023-07-18T11:27:42.665160Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class ClassConfig:\n    def __init__(self, dropout=0.2, num_labels=3):\n        self.dropout_rate = dropout\n        self.num_labels = num_labels\n\nclass T5EncoderForTokenClassification(T5PreTrainedModel):\n\n    def __init__(self, config: T5Config, class_config):\n        super().__init__(config)\n        self.num_labels = class_config.num_labels\n        self.config = config\n\n        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n\n        encoder_config = copy.deepcopy(config)\n        encoder_config.use_cache = False\n        encoder_config.is_encoder_decoder = False\n        self.encoder = T5Stack(encoder_config, self.shared)\n\n        self.dropout = nn.Dropout(class_config.dropout_rate) \n        self.classifier = nn.Linear(config.hidden_size, class_config.num_labels)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n        # Model parallel\n        self.model_parallel = False\n        self.device_map = None\n\n    def parallelize(self, device_map=None):\n        self.device_map = (\n            get_device_map(len(self.encoder.block), range(torch.cuda.device_count()))\n            if device_map is None\n            else device_map\n        )\n        assert_device_map(self.device_map, len(self.encoder.block))\n        self.encoder.parallelize(self.device_map)\n        self.classifier = self.classifier.to(self.encoder.first_device)\n        self.model_parallel = True\n\n    def deparallelize(self):\n        self.encoder.deparallelize()\n        self.encoder = self.encoder.to(\"cpu\")\n        self.model_parallel = False\n        self.device_map = None\n        torch.cuda.empty_cache()\n\n    def get_input_embeddings(self):\n        return self.shared\n\n    def set_input_embeddings(self, new_embeddings):\n        self.shared = new_embeddings\n        self.encoder.set_input_embeddings(new_embeddings)\n\n    def get_encoder(self):\n        return self.encoder\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.encoder(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            inputs_embeds=inputs_embeds,\n            head_mask=head_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n\n            active_loss = attention_mask.view(-1) == 1\n            active_logits = logits.view(-1, self.num_labels)\n\n            active_labels = torch.where(\n              active_loss, labels.view(-1), torch.tensor(-100).type_as(labels)\n            )\n\n            valid_logits=active_logits[active_labels!=-100]\n            valid_labels=active_labels[active_labels!=-100]\n            \n            valid_labels=valid_labels.type(torch.LongTensor).to('cuda:0')\n            \n            loss = loss_fct(valid_logits, valid_labels)\n            \n        \n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )","metadata":{"execution":{"iopub.status.busy":"2023-07-18T11:27:42.667847Z","iopub.execute_input":"2023-07-18T11:27:42.668274Z","iopub.status.idle":"2023-07-18T11:27:42.693656Z","shell.execute_reply.started":"2023-07-18T11:27:42.668234Z","shell.execute_reply":"2023-07-18T11:27:42.692371Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def PT5_classification_model(num_labels):\n    # Load PT5 and tokenizer\n    model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n    tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\") \n    \n    # Create new Classifier model with PT5 dimensions\n    class_config=ClassConfig(num_labels=num_labels)\n    class_model=T5EncoderForTokenClassification(model.config,class_config)\n    \n    # Set encoder and embedding weights to checkpoint weights\n    class_model.shared=model.shared\n    class_model.encoder=model.encoder    \n    \n    # Delete the checkpoint model\n    model=class_model\n    del class_model\n    \n    # Print number of trainable parameters\n    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n    params = sum([np.prod(p.size()) for p in model_parameters])\n    print(\"ProtT5_Classfier\\nTrainable Parameter: \"+ str(params))    \n \n    # Add model modification lora\n    config = LoRAConfig()\n    \n    # Add LoRA layers\n    model = modify_with_lora(model, config)\n    \n    # Freeze Embeddings and Encoder (except LoRA)\n    for (param_name, param) in model.shared.named_parameters():\n                param.requires_grad = False\n    for (param_name, param) in model.encoder.named_parameters():\n                param.requires_grad = False       \n\n    for (param_name, param) in model.named_parameters():\n            if re.fullmatch(config.trainable_param_names, param_name):\n                param.requires_grad = True\n\n    # Print trainable Parameter          \n    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n    params = sum([np.prod(p.size()) for p in model_parameters])\n    print(\"ProtT5_LoRA_Classfier\\nTrainable Parameter: \"+ str(params) + \"\\n\")\n    \n    return model, tokenizer","metadata":{"execution":{"iopub.status.busy":"2023-07-18T11:27:42.695944Z","iopub.execute_input":"2023-07-18T11:27:42.696751Z","iopub.status.idle":"2023-07-18T11:27:42.717162Z","shell.execute_reply.started":"2023-07-18T11:27:42.696706Z","shell.execute_reply":"2023-07-18T11:27:42.715925Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Deepspeed config for optimizer CPU offload\n\nds_config = {\n    \"fp16\": {\n        \"enabled\": \"auto\",\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    },\n\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": \"auto\",\n            \"betas\": \"auto\",\n            \"eps\": \"auto\",\n            \"weight_decay\": \"auto\"\n        }\n    },\n\n    \"scheduler\": {\n        \"type\": \"WarmupLR\",\n        \"params\": {\n            \"warmup_min_lr\": \"auto\",\n            \"warmup_max_lr\": \"auto\",\n            \"warmup_num_steps\": \"auto\"\n        }\n    },\n\n    \"zero_optimization\": {\n        \"stage\": 2,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": True\n        },\n        \"allgather_partitions\": True,\n        \"allgather_bucket_size\": 2e8,\n        \"overlap_comm\": True,\n        \"reduce_scatter\": True,\n        \"reduce_bucket_size\": 2e8,\n        \"contiguous_gradients\": True\n    },\n\n    \"gradient_accumulation_steps\": \"auto\",\n    \"gradient_clipping\": \"auto\",\n    \"steps_per_print\": 2000,\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"wall_clock_breakdown\": False\n}","metadata":{"execution":{"iopub.status.busy":"2023-07-18T11:27:42.721430Z","iopub.execute_input":"2023-07-18T11:27:42.721831Z","iopub.status.idle":"2023-07-18T11:27:42.739676Z","shell.execute_reply.started":"2023-07-18T11:27:42.721798Z","shell.execute_reply":"2023-07-18T11:27:42.738377Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Set random seeds for reproducibility of your trainings run\ndef set_seeds(s):\n    torch.manual_seed(s)\n    np.random.seed(s)\n    random.seed(s)\n    set_seed(s)\n\n# Dataset creation\ndef create_dataset(tokenizer,seqs,labels):\n    tokenized = tokenizer(seqs, max_length=1024, padding=True, truncation=True)\n    dataset = Dataset.from_dict(tokenized)\n    # we need to cut of labels after 1023 positions for the data collator to add the correct padding (1023 + 1 special tokens)\n    labels = [l[:1023] for l in labels] \n    dataset = dataset.add_column(\"labels\", labels)\n     \n    return dataset\n    \n# Main training fuction\ndef train_per_residue(\n        train_df,         #training data\n        valid_df,         #validation data      \n        num_labels= 3,    #number of classes\n    \n        # effective training batch size is batch * accum\n        # we recommend an effective batch size of 8 \n        batch= 4,         #for training\n        accum= 2,         #gradient accumulation\n    \n        val_batch = 16,   #batch size for evaluation\n        epochs= 10,       #training epochs\n        lr= 3e-4,         #recommended learning rate\n        seed= 42,         #random seed\n        deepspeed= True,  #if gpu is large enough disable deepspeed for training speedup\n        gpu= 1 ):         #gpu selection (1 for first gpu)\n\n    # Set gpu device\n    os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(gpu-1)\n    \n    # Set all random seeds\n    set_seeds(seed)\n    \n    # load model\n    model, tokenizer = PT5_classification_model(num_labels=num_labels)\n\n    # Preprocess inputs\n    # Replace uncommon AAs with \"X\"\n    train_df[\"sequence\"]=train_df[\"sequence\"].str.replace('|'.join([\"O\",\"B\",\"U\",\"Z\"]),\"X\",regex=True)\n    valid_df[\"sequence\"]=valid_df[\"sequence\"].str.replace('|'.join([\"O\",\"B\",\"U\",\"Z\"]),\"X\",regex=True)\n    # Add spaces between each amino acid for PT5 to correctly use them\n    train_df['sequence']=train_df.apply(lambda row : \" \".join(row[\"sequence\"]), axis = 1)\n    valid_df['sequence']=valid_df.apply(lambda row : \" \".join(row[\"sequence\"]), axis = 1)\n\n\n    # Create Datasets\n    train_set=create_dataset(tokenizer,list(train_df['sequence']),list(train_df['label']))\n    valid_set=create_dataset(tokenizer,list(valid_df['sequence']),list(valid_df['label']))\n\n    # Huggingface Trainer arguments\n    args = TrainingArguments(\n        \"./scripts/Finetuning/PT5/\",\n        evaluation_strategy = \"steps\",\n        eval_steps = 500,\n        logging_strategy = \"epoch\",\n        save_strategy = \"no\",\n        learning_rate=lr,\n        per_device_train_batch_size=batch,\n        #per_device_eval_batch_size=val_batch,\n        per_device_eval_batch_size=batch,\n        gradient_accumulation_steps=accum,\n        num_train_epochs=epochs,\n        seed = seed,\n        deepspeed= ds_config if deepspeed else None,\n    ) \n\n    # Metric definition for validation data\n    def compute_metrics(eval_pred):\n\n        metric = load(\"accuracy\")\n        predictions, labels = eval_pred\n        \n        labels = labels.reshape((-1,))\n        \n        predictions = np.argmax(predictions, axis=2)\n        predictions = predictions.reshape((-1,))\n        \n        predictions = predictions[labels!=-100]\n        labels = labels[labels!=-100]\n        \n        return metric.compute(predictions=predictions, references=labels)\n\n    # For token classification we need a data collator here to pad correctly\n    data_collator = DataCollatorForTokenClassification(tokenizer) \n\n    # Trainer          \n    trainer = Trainer(\n        model,\n        args,\n        train_dataset=train_set,\n        eval_dataset=valid_set,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics\n    )    \n    \n    # Train model\n    trainer.train()\n\n    return tokenizer, model, trainer.state.log_history","metadata":{"execution":{"iopub.status.busy":"2023-07-18T11:27:42.741795Z","iopub.execute_input":"2023-07-18T11:27:42.742234Z","iopub.status.idle":"2023-07-18T11:27:42.768903Z","shell.execute_reply.started":"2023-07-18T11:27:42.742185Z","shell.execute_reply":"2023-07-18T11:27:42.767499Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"tokenizer, model, history = train_per_residue(my_train, my_valid, num_labels=3, batch=1, accum=1, epochs=1, seed=42, gpu=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get loss, val_loss, and the computed metric from history\nloss = [x['loss'] for x in history if 'loss' in x]\nval_loss = [x['eval_loss'] for x in history if 'eval_loss' in x]\n\n# Get accuracy value \nmetric = [x['eval_accuracy'] for x in history if 'eval_accuracy' in x]\n\nepochs_loss = [x['epoch'] for x in history if 'loss' in x]\nepochs_eval = [x['epoch'] for x in history if 'eval_loss' in x]\n\n# Create a figure with two y-axes\nfig, ax1 = plt.subplots(figsize=(10, 5))\nax2 = ax1.twinx()\n\n# Plot loss and val_loss on the first y-axis\n# For the loss we plot a horizontal line because we have just one loss value (after the first epoch)\n# Exchange the two lines below if you trained multiple epochs\nline1 = ax1.plot([0]+epochs_loss, loss*2, label='train_loss')\n#line1 = ax1.plot(epochs_loss, loss, label='train_loss')\n\nline2 = ax1.plot(epochs_eval, val_loss, label='val_loss')\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss')\n\n# Plot the computed metric on the second y-axis\nline3 = ax2.plot(epochs_eval, metric, color='red', label='val_accuracy')\nax2.set_ylabel('Accuracy')\nax2.set_ylim([0, 1])\n\n# Combine the lines from both y-axes and create a single legend\nlines = line1 + line2 + line3\nlabels = [line.get_label() for line in lines]\nax1.legend(lines, labels, loc='lower left')\n\n# Show the plot\nplt.title(\"Training History\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_model(model,filepath):\n# Saves all parameters that were changed during finetuning\n\n    # Create a dictionary to hold the non-frozen parameters\n    non_frozen_params = {}\n\n    # Iterate through all the model parameters\n    for param_name, param in model.named_parameters():\n        # If the parameter has requires_grad=True, add it to the dictionary\n        if param.requires_grad:\n            non_frozen_params[param_name] = param\n\n    # Save only the finetuned parameters \n    torch.save(non_frozen_params, filepath)\n\n    \ndef load_model(filepath, num_labels=1):\n# Creates a new PT5 model and loads the finetuned weights from a file\n\n    # load a new model\n    model, tokenizer = PT5_classification_model(num_labels=num_labels)\n    \n    # Load the non-frozen parameters from the saved file\n    non_frozen_params = torch.load(filepath)\n\n    # Assign the non-frozen parameters to the corresponding parameters of the model\n    for param_name, param in model.named_parameters():\n        if param_name in non_frozen_params:\n            param.data = non_frozen_params[param_name].data\n\n    return tokenizer, model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_model(model,\"./PT5_secstr_finetuned.pth\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop unneeded columns (remember, mask was already included as -100 values to label)\nmy_test=my_test[[\"sequence\",\"label\"]]\n\n# Preprocess sequences\nmy_test[\"sequence\"]=my_test[\"sequence\"].str.replace('|'.join([\"O\",\"B\",\"U\",\"Z\"]),\"X\",regex=True)\nmy_test['sequence']=my_test.apply(lambda row : \" \".join(row[\"sequence\"]), axis = 1)\nmy_test.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the device to use\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)\n\n# Create Dataset\ntest_set=create_dataset(tokenizer,list(my_test['sequence']),list(my_test['label']))\n# Make compatible with torch DataLoader\ntest_set = test_set.with_format(\"torch\", device=device)\n\n# For token classification we need a data collator here to pad correctly\ndata_collator = DataCollatorForTokenClassification(tokenizer) \n\n# Create a dataloader for the test dataset\ntest_dataloader = DataLoader(test_set, batch_size=16, shuffle = False, collate_fn = data_collator)\n\n# Put the model in evaluation mode\nmodel.eval()\n\n# Make predictions on the test dataset\npredictions = []\n# We need to collect the batch[\"labels\"] as well, this allows us to filter out all positions with a -100 afterwards\npadded_labels = []\n\nwith torch.no_grad():\n    for batch in tqdm(test_dataloader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        # Padded labels from the data collator\n        padded_labels += batch['labels'].tolist()\n        # Add batch results(logits) to predictions, we take the argmax here to get the predicted class\n        predictions += model(input_ids, attention_mask=attention_mask).logits.argmax(dim=-1).tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to make it easier we flatten both the label and prediction lists\ndef flatten(l):\n    return [item for sublist in l for item in sublist]\n\n# flatten and convert to np array for easy slicing in the next step\npredictions = np.array(flatten(predictions))\npadded_labels = np.array(flatten(padded_labels))\n\n# Filter out all invalid (label = -100) values\npredictions = predictions[padded_labels!=-100]\npadded_labels = padded_labels[padded_labels!=-100]\n\n# Calculate classification Accuracy\nprint(\"Accuracy: \", accuracy_score(padded_labels, predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}