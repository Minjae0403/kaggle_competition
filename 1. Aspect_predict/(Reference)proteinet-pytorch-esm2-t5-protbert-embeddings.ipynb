{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CAFA 5 Competition : Protein Function Prediction","metadata":{}},{"cell_type":"markdown","source":"# Problem Framing","metadata":{}},{"cell_type":"markdown","source":"This Kaggle competition aims to predict the function of proteins using their amino-acid sequences and additional data. Understanding protein function is crucial for comprehending cellular processes and developing new treatments for diseases. With the abundance of genomic sequence data available, assigning accurate biological functions to proteins becomes challenging due to their multifunctionality and interactions with various partners. This competition, hosted by the Function Community of Special Interest (Function-COSI), brings together computational biologists, experimental biologists, and biocurators to improve protein function prediction through data science and machine learning approaches. The goal is to contribute to advancements in medicine, agriculture, and overall human and animal health.","metadata":{}},{"cell_type":"markdown","source":"![image-intro](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4a/Computational_solvent_mapping_of_AMA1_using_FTMAP.TIF/lossy-page1-500px-Computational_solvent_mapping_of_AMA1_using_FTMAP.TIF.jpg)","metadata":{}},{"cell_type":"markdown","source":"# What to submit ?","metadata":{}},{"cell_type":"markdown","source":"This competition evaluates participants' predictions of Gene Ontology (GO) terms for protein sequences. The evaluation is performed on a test set of proteins that initially have no assigned functions but may accumulate experimental annotations after the submission deadline. The test set is divided into three subontologies: Molecular Function (MF), Biological Process (BP), and Cellular Component (CC). Participants are scored separately for each subontology. The final performance measure is the arithmetic mean of the maximum F-measures calculated on the three subontologies. Weighted precision and recall are used, taking into account the hierarchical structure of the GO. The evaluation code is publicly available. The leaderboard displays performance on a subset of proteins not included in the final test set, so generalization performance is crucial. Submission files should contain protein-target pairs with corresponding GO terms and estimated probabilities, within a specific score range. The predictions are propagated to parent terms if not explicitly listed. There is a limit on the number of terms associated with each protein. If a protein is not listed in the submission file, it is assumed that all predictions for that protein are zero.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nsub = pd.read_csv(\"/kaggle/input/cafa-5-protein-function-prediction/sample_submission.tsv\", sep= \"\\t\", header = None)\nsub.columns = [\"The Protein ID\", \"The Gene Ontology term (GO) ID\", \"Predicted link probability that GO appear in Protein\"]\nsub.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-06-27T11:52:25.078568Z","iopub.execute_input":"2023-06-27T11:52:25.079184Z","iopub.status.idle":"2023-06-27T11:52:25.355999Z","shell.execute_reply.started":"2023-06-27T11:52:25.079147Z","shell.execute_reply":"2023-06-27T11:52:25.354925Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"  The Protein ID The Gene Ontology term (GO) ID  \\\n0     A0A0A0MRZ7                     GO:0000001   \n1     A0A0A0MRZ7                     GO:0000002   \n2     A0A0A0MRZ8                     GO:0000001   \n3     A0A0A0MRZ8                     GO:0000002   \n4     A0A0A0MRZ9                     GO:0000001   \n\n   Predicted link probability that GO appear in Protein  \n0                                              0.123     \n1                                              0.123     \n2                                              0.123     \n3                                              0.123     \n4                                              0.123     ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>The Protein ID</th>\n      <th>The Gene Ontology term (GO) ID</th>\n      <th>Predicted link probability that GO appear in Protein</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A0A0A0MRZ7</td>\n      <td>GO:0000001</td>\n      <td>0.123</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A0A0A0MRZ7</td>\n      <td>GO:0000002</td>\n      <td>0.123</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A0A0A0MRZ8</td>\n      <td>GO:0000001</td>\n      <td>0.123</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A0A0A0MRZ8</td>\n      <td>GO:0000002</td>\n      <td>0.123</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A0A0A0MRZ9</td>\n      <td>GO:0000001</td>\n      <td>0.123</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"The evaluation metric used in this competition is the weighted F-measure, which combines precision and recall, taking into account the hierarchical structure of the Gene Ontology (GO). The formula for calculating the weighted F-measure is as follows:\n\n$Weighted \\ F-measure = \\frac{(1 + β^2) * (weighted \\ precision * weighted recall)}{((β^2 * weighted \\ precision) + weighted \\ recall)}$\n\nwhere:\n- β is a parameter that controls the trade-off between precision and recall. In this competition, β is set to 1, resulting in an equal weighting of precision and recall.\n- Weighted precision is the precision score, considering the weights of the predicted terms. It is calculated as the sum of the products of the predicted term's weight and its true positive count, divided by the sum of the weights of all predicted terms.\n- Weighted recall is the recall score, considering the weights of the true positive terms. It is calculated as the sum of the products of the true positive term's weight and its count, divided by the sum of the weights of all true positive terms.\n\nNote: The specific formulas for weighted precision and weighted recall are provided in the competition materials and utilize additional information such as term weights and true positive counts.","metadata":{}},{"cell_type":"markdown","source":"# General Baseline in this Notebook","metadata":{}},{"cell_type":"markdown","source":"- **1 - Collect Embedding vectors from pre-trained protein function prediction models (T5, ProtBERT or EMS2) :**\n\nSources for embeddings vectors : \n- *T5* : https://www.kaggle.com/datasets/sergeifironov/t5embeds\n\n- *ProtBERT* : https://www.kaggle.com/datasets/henriupton/protbert-embeddings-for-cafa5\n\n- *EMS2* : https://www.kaggle.com/datasets/viktorfairuschin/cafa-5-ems-2-embeddings-numpy\n\n- **2 - Generate labels from train_terms file** : by considering the top K most common GO terms in all Proteins set, generate for each protein a sparse vector of length K to indicate the true probabilities that each of the K GO terms are in the Protein (0 or 1). Here we retain K = 600\n\n- **3 - Create Pytorch Dataset class that can handle Protein ID and embeddings**.\n\n- **4 - Create Pytorch Model class for prediction** : can be any architecture of Multilabel classification model that can turn embeddings of shape (E,) to probabilities of shape (K,). Here we explore **MultiLayerPerceptron** and **ConvNN1d** Networks.\n\n- **5 - Make Cross Validation w.r.t the F-1 measure and do Hyperparameter tuning thanks to Weights and Biases package (Wandb)**","metadata":{}},{"cell_type":"markdown","source":"![baseline-image](https://www.researchgate.net/publication/334642149/figure/fig1/AS:783995214249986@1563930433525/Flow-chart-of-STRING2GO-based-protein-function-prediction-method.png)\n\n## **Flow-chart of STRING2GO-based protein function prediction method**","metadata":{}},{"cell_type":"markdown","source":"# So now lets get started !","metadata":{}},{"cell_type":"markdown","source":"# 1. Imports / Config","metadata":{}},{"cell_type":"code","source":"MAIN_DIR = \"/kaggle/input/cafa-5-protein-function-prediction\"\n\n# UTILITARIES\nimport numpy as np\nfrom tqdm import tqdm\nimport time\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\n# TORCH MODULES FOR METRICS COMPUTATION :\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch import nn\nfrom torch.utils.data import random_split\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torchmetrics.classification import MultilabelF1Score\nfrom torchmetrics.classification import MultilabelAccuracy\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.loggers import WandbLogger\n\n# WANDB FOR LIGHTNING :\nimport wandb\n\n# FILES VISUALIZATION\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-27T11:53:52.290912Z","iopub.execute_input":"2023-06-27T11:53:52.291301Z","iopub.status.idle":"2023-06-27T11:54:05.585200Z","shell.execute_reply.started":"2023-06-27T11:53:52.291271Z","shell.execute_reply":"2023-06-27T11:54:05.584291Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/cafa-5-protein-function-prediction/sample_submission.tsv\n/kaggle/input/cafa-5-protein-function-prediction/IA.txt\n/kaggle/input/cafa-5-protein-function-prediction/Test (Targets)/testsuperset.fasta\n/kaggle/input/cafa-5-protein-function-prediction/Test (Targets)/testsuperset-taxon-list.tsv\n/kaggle/input/cafa-5-protein-function-prediction/Train/train_terms.tsv\n/kaggle/input/cafa-5-protein-function-prediction/Train/train_sequences.fasta\n/kaggle/input/cafa-5-protein-function-prediction/Train/train_taxonomy.tsv\n/kaggle/input/cafa-5-protein-function-prediction/Train/go-basic.obo\n/kaggle/input/t5embeds/train_ids.npy\n/kaggle/input/t5embeds/test_embeds.npy\n/kaggle/input/t5embeds/train_embeds.npy\n/kaggle/input/t5embeds/test_ids.npy\n/kaggle/input/protbert-embeddings-for-cafa5/train_ids.npy\n/kaggle/input/protbert-embeddings-for-cafa5/train_embeddings.npy\n/kaggle/input/protbert-embeddings-for-cafa5/test_ids.npy\n/kaggle/input/protbert-embeddings-for-cafa5/test_embeddings.npy\n/kaggle/input/cafa-5-ems-2-embeddings-numpy/train_ids.npy\n/kaggle/input/cafa-5-ems-2-embeddings-numpy/train_embeddings.npy\n/kaggle/input/cafa-5-ems-2-embeddings-numpy/test_ids.npy\n/kaggle/input/cafa-5-ems-2-embeddings-numpy/test_embeddings.npy\n","output_type":"stream"}]},{"cell_type":"code","source":"class config:\n    train_sequences_path = MAIN_DIR  + \"/Train/train_sequences.fasta\"\n    train_labels_path = MAIN_DIR + \"/Train/train_terms.tsv\"\n    test_sequences_path = MAIN_DIR + \"/Test (Targets)/testsuperset.fasta\"\n    \n    num_labels = 500\n    n_epochs = 5\n    batch_size = 128\n    lr = 0.001\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2023-06-27T11:54:05.590345Z","iopub.execute_input":"2023-06-27T11:54:05.593624Z","iopub.status.idle":"2023-06-27T11:54:05.612299Z","shell.execute_reply.started":"2023-06-27T11:54:05.593584Z","shell.execute_reply":"2023-06-27T11:54:05.611229Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print(config.device)","metadata":{"execution":{"iopub.status.busy":"2023-06-27T11:54:09.800555Z","iopub.execute_input":"2023-06-27T11:54:09.800919Z","iopub.status.idle":"2023-06-27T11:54:09.807176Z","shell.execute_reply.started":"2023-06-27T11:54:09.800890Z","shell.execute_reply":"2023-06-27T11:54:09.806179Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 2. Baseline","metadata":{}},{"cell_type":"markdown","source":"- Use ProtBERT/T5 embedding vectors for each Sequence ID and its associated Protein Sequence. \n\n- Define a Pytorch Dataset to load all ids/sequences of the train+test sets and their respective ProtBert embeddings\n\n- Define Pytorch Model architecture in order to use these embeddings to proceed classification task : to each ID we associate a probability to be associated to each GO term **For this part, just consider the top_n most common GO terms as labels**\n\n- Return desired probabilities for test set","metadata":{}},{"cell_type":"markdown","source":"# 3. Collect ProtBERT Embedding Vectors","metadata":{}},{"cell_type":"markdown","source":"### Here is a Dataset Card I have created if you want to understand better what are pre-trained model embedding vectors, and if you want to download it directly from it : https://www.kaggle.com/datasets/henriupton/protbert-embeddings-for-cafa5","metadata":{}},{"cell_type":"markdown","source":"##### SCRIPT FOR VECTOR EMBEDDINGS COLLECTING #####\n#### RUN THIS ONLY ONE TIME AND SAVE IT IN LOCAL ####\n\n```python\n\nprint(\"Load ProtBERT Model...\")\n# PROT BERT LOADING :\nfrom transformers import BertModel, BertTokenizer\ntokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\nmodel = BertModel.from_pretrained(\"Rostlab/prot_bert\").to(config.device)\n\ndef get_bert_embedding(\n    sequence : str,\n    len_seq_limit : int\n):\n    '''\n    Function to collect last hidden state embedding vector from pre-trained ProtBERT Model\n    \n    INPUTS:\n    - sequence (str) : protein sequence (ex : AAABBB) from fasta file\n    - len_seq_limit (int) : maximum sequence lenght (i.e nb of letters) for truncation\n    \n    OUTPUTS:\n    - output_hidden : last hidden state embedding vector for input sequence of length 1024\n    '''\n    sequence_w_spaces = ' '.join(list(sequence))\n    encoded_input = tokenizer(\n        sequence_w_spaces,\n        truncation=True,\n        max_length=len_seq_limit,\n        padding='max_length',\n        return_tensors='pt').to(config.device)\n    output = model(**encoded_input)\n    output_hidden = output['last_hidden_state'][:,0][0].detach().cpu().numpy()\n    assert len(output_hidden)==1024\n    return output_hidden\n\n### COLLECTING FOR TRAIN SAMPLES :\nprint(\"Loading train set ProtBERT Embeddings...\")\nfasta_train = SeqIO.parse(config.train_sequences_path, \"fasta\")\nprint(\"Total Nb of Elements : \", len(list(fasta_train)))\nfasta_train = SeqIO.parse(config.train_sequences_path, \"fasta\")\nids_list = []\nembed_vects_list = []\nt0 = time.time()\ncheckpoint = 0\nfor item in tqdm(fasta_train):\n    ids_list.append(item.id)\n    embed_vects_list.append(\n        get_bert_embedding(sequence = item.seq, len_seq_limit = 1200))\n    checkpoint+=1\n    if checkpoint>=100:\n        df_res = pd.DataFrame(data={\"id\" : ids_list, \"embed_vect\" : embed_vects_list})\n        np.save('/kaggle/working/train_ids.npy',np.array(ids_list))\n        np.save('/kaggle/working/train_embeddings.npy',np.array(embed_vects_list))\n        checkpoint=0\n        \nnp.save('/kaggle/working/train_ids.npy',np.array(ids_list))\nnp.save('/kaggle/working/train_embeddings.npy',np.array(embed_vects_list))\nprint('Total Elapsed Time:',time.time()-t0)\n\n### COLLECTING FOR TEST SAMPLES :\nprint(\"Loading test set ProtBERT Embeddings...\")\nfasta_test = SeqIO.parse(config.test_sequences_path, \"fasta\")\nprint(\"Total Nb of Elements : \", len(list(fasta_test)))\nfasta_test = SeqIO.parse(config.test_sequences_path, \"fasta\")\nids_list = []\nembed_vects_list = []\nt0 = time.time()\ncheckpoint=0\nfor item in tqdm(fasta_test):\n    ids_list.append(item.id)\n    embed_vects_list.append(\n        get_bert_embedding(sequence = item.seq, len_seq_limit = 1200))\n    checkpoint+=1\n    if checkpoint>=100:\n        np.save('/kaggle/working/test_ids.npy',np.array(ids_list))\n        np.save('/kaggle/working/test_embeddings.npy',np.array(embed_vects_list))\n        checkpoint=0\n        \nnp.save('/kaggle/working/test_ids.npy',np.array(ids_list))\nnp.save('/kaggle/working/test_embeddings.npy',np.array(embed_vects_list))\nprint('Total Elasped Time:',time.time()-t0)\n```","metadata":{"execution":{"iopub.status.busy":"2023-05-05T08:54:24.395576Z","iopub.execute_input":"2023-05-05T08:54:24.396067Z","iopub.status.idle":"2023-05-05T08:54:35.014197Z","shell.execute_reply.started":"2023-05-05T08:54:24.396028Z","shell.execute_reply":"2023-05-05T08:54:35.012847Z"}}},{"cell_type":"code","source":"# PROT BERT LOADING :\nfrom transformers import BertModel, BertTokenizer\ntokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\nmodel = BertModel.from_pretrained(\"Rostlab/prot_bert\").to(config.device)\n\ndef get_bert_embedding(\n    sequence : str,\n    len_seq_limit : int\n):\n    '''\n    Function to collect last hidden state embedding vector from pre-trained ProtBERT Model\n    \n    INPUTS:\n    - sequence (str) : protein sequence (ex : AAABBB) from fasta file\n    - len_seq_limit (int) : maximum sequence lenght (i.e nb of letters) for truncation\n    \n    OUTPUTS:\n    - output_hidden : last hidden state embedding vector for input sequence of length 1024\n    '''\n    sequence_w_spaces = ' '.join(list(sequence))\n    encoded_input = tokenizer(\n        sequence_w_spaces,\n        truncation=True,\n        max_length=len_seq_limit,\n        padding='max_length',\n        return_tensors='pt').to(config.device)\n    output = model(**encoded_input)\n    output_hidden = output['last_hidden_state'][:,0][0].detach().cpu().numpy()\n    assert len(output_hidden)==1024\n    return output_hidden\n\n### COLLECTING FOR TRAIN SAMPLES :\nprint(\"Loading train set ProtBERT Embeddings...\")\nfasta_train = SeqIO.parse(config.train_sequences_path, \"fasta\")\nprint(\"Total Nb of Elements : \", len(list(fasta_train)))\nfasta_train = SeqIO.parse(config.train_sequences_path, \"fasta\")\nids_list = []\nembed_vects_list = []\nt0 = time.time()\ncheckpoint = 0\nfor item in tqdm(fasta_train):\n    ids_list.append(item.id)\n    embed_vects_list.append(\n        get_bert_embedding(sequence = item.seq, len_seq_limit = 1200))\n    checkpoint+=1\n    if checkpoint>=100:\n        df_res = pd.DataFrame(data={\"id\" : ids_list, \"embed_vect\" : embed_vects_list})\n        np.save('/kaggle/working/train_ids.npy',np.array(ids_list))\n        np.save('/kaggle/working/train_embeddings.npy',np.array(embed_vects_list))\n        checkpoint=0\n        \nnp.save('/kaggle/working/train_ids.npy',np.array(ids_list))\nnp.save('/kaggle/working/train_embeddings.npy',np.array(embed_vects_list))\nprint('Total Elapsed Time:',time.time()-t0)\n\n### COLLECTING FOR TEST SAMPLES :\nprint(\"Loading test set ProtBERT Embeddings...\")\nfasta_test = SeqIO.parse(config.test_sequences_path, \"fasta\")\nprint(\"Total Nb of Elements : \", len(list(fasta_test)))\nfasta_test = SeqIO.parse(config.test_sequences_path, \"fasta\")\nids_list = []\nembed_vects_list = []\nt0 = time.time()\ncheckpoint=0\nfor item in tqdm(fasta_test):\n    ids_list.append(item.id)\n    embed_vects_list.append(\n        get_bert_embedding(sequence = item.seq, len_seq_limit = 1200))\n    checkpoint+=1\n    if checkpoint>=100:\n        np.save('/kaggle/working/test_ids.npy',np.array(ids_list))\n        np.save('/kaggle/working/test_embeddings.npy',np.array(embed_vects_list))\n        checkpoint=0\n        \nnp.save('/kaggle/working/test_ids.npy',np.array(ids_list))\nnp.save('/kaggle/working/test_embeddings.npy',np.array(embed_vects_list))\nprint('Total Elasped Time:',time.time()-t0)","metadata":{"execution":{"iopub.status.busy":"2023-06-27T11:55:41.565372Z","iopub.execute_input":"2023-06-27T11:55:41.565717Z","iopub.status.idle":"2023-06-27T11:56:02.414359Z","shell.execute_reply.started":"2023-06-27T11:55:41.565689Z","shell.execute_reply":"2023-06-27T11:56:02.413082Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/81.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae994f10270d4a849ec6a9d2a93edeb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"165b039e6f194fd9bb7b5fccb8bd6e3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/86.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a0dbb11a8234fe9bfabf42ac92ba10b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/361 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"398dea8821fe4889a087625924b6e4b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/1.68G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80e10158ceba4a8f958ebabe56f0d117"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"Loading train set ProtBERT Embeddings...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m### COLLECTING FOR TRAIN SAMPLES :\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading train set ProtBERT Embeddings...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m fasta_train \u001b[38;5;241m=\u001b[39m \u001b[43mSeqIO\u001b[49m\u001b[38;5;241m.\u001b[39mparse(config\u001b[38;5;241m.\u001b[39mtrain_sequences_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfasta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Nb of Elements : \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(fasta_train)))\n\u001b[1;32m     36\u001b[0m fasta_train \u001b[38;5;241m=\u001b[39m SeqIO\u001b[38;5;241m.\u001b[39mparse(config\u001b[38;5;241m.\u001b[39mtrain_sequences_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfasta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'SeqIO' is not defined"],"ename":"NameError","evalue":"name 'SeqIO' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"# 4. Collect labels vectors for train/test","metadata":{}},{"cell_type":"markdown","source":"##### SCRIPT FOR LABELS (TARGETS) VECTORS COLLECTING #####\n#### RUN THIS ONLY ONE TIME AND SAVE IT IN LOCAL ####\n\n```python\nprint(\"GENERATE TARGETS FOR ENTRY IDS (\"+str(config.num_labels)+\" MOST COMMON GO TERMS)\")\nids = np.load(\"/kaggle/input/protbert-embeddings-for-cafa5/train_ids.npy\")\nlabels = pd.read_csv(config.train_labels_path, sep = \"\\t\")\n\ntop_terms = labels.groupby(\"term\")[\"EntryID\"].count().sort_values(ascending=False)\nlabels_names = top_terms[:config.num_labels].index.values\ntrain_labels_sub = labels[(labels.term.isin(labels_names)) & (labels.EntryID.isin(ids))]\nid_labels = train_labels_sub.groupby('EntryID')['term'].apply(list).to_dict()\n\ngo_terms_map = {label: i for i, label in enumerate(labels_names)}\nlabels_matrix = np.empty((len(ids), len(labels_names)))\n\nfor index, id in tqdm(enumerate(ids)):\n    id_gos_list = id_labels[id]\n    temp = [go_terms_map[go] for go in labels_names if go in id_gos_list]\n    labels_matrix[index, temp] = 1\n\nnp.save(\"/kaggle/working/train_targets_top\"+str(config.num_labels)+\".npy\", np.array(labels_matrix))\nprint(\"GENERATION FINISHED!\")\n```","metadata":{"execution":{"iopub.status.busy":"2023-05-11T17:45:22.690593Z","iopub.execute_input":"2023-05-11T17:45:22.691183Z","iopub.status.idle":"2023-05-11T17:45:24.979748Z","shell.execute_reply.started":"2023-05-11T17:45:22.691147Z","shell.execute_reply":"2023-05-11T17:45:24.978075Z"}}},{"cell_type":"markdown","source":"# 5. Pytorch Dataset Architecture","metadata":{}},{"cell_type":"code","source":"# Directories for the different embedding vectors : \nembeds_map = {\n    \"T5\" : \"t5embeds\",\n    \"ProtBERT\" : \"protbert-embeddings-for-cafa5\",\n    \"EMS2\" : \"cafa-5-ems-2-embeddings-numpy\"\n}\n\n# Length of the different embedding vectors :\nembeds_dim = {\n    \"T5\" : 1024,\n    \"ProtBERT\" : 1024,\n    \"EMS2\" : 1280\n}","metadata":{"execution":{"iopub.status.busy":"2023-05-17T14:52:07.484476Z","iopub.execute_input":"2023-05-17T14:52:07.485411Z","iopub.status.idle":"2023-05-17T14:52:07.490852Z","shell.execute_reply.started":"2023-05-17T14:52:07.485344Z","shell.execute_reply":"2023-05-17T14:52:07.48972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ProteinSequenceDataset(Dataset):\n    \n    def __init__(self, datatype, embeddings_source):\n        super(ProteinSequenceDataset).__init__()\n        self.datatype = datatype\n        \n        if embeddings_source in [\"ProtBERT\", \"EMS2\"]:\n            embeds = np.load(\"/kaggle/input/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_embeddings.npy\")\n            ids = np.load(\"/kaggle/input/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_ids.npy\")\n        \n        if embeddings_source == \"T5\":\n            embeds = np.load(\"/kaggle/input/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_embeds.npy\")\n            ids = np.load(\"/kaggle/input/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_ids.npy\")\n            \n        embeds_list = []\n        for l in range(embeds.shape[0]):\n            embeds_list.append(embeds[l,:])\n        self.df = pd.DataFrame(data={\"EntryID\": ids, \"embed\" : embeds_list})\n        \n        if datatype==\"train\":\n            df_labels = pd.read_pickle(\n                \"/kaggle/input/train-targets-top\"+str(config.num_labels)+ \\\n                \"/train_targets_top\"+str(config.num_labels)+\".pkl\")\n            self.df = self.df.merge(df_labels, on=\"EntryID\")\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        embed = torch.tensor(self.df.iloc[index][\"embed\"] , dtype = torch.float32)\n        if self.datatype==\"train\":\n            targets = torch.tensor(self.df.iloc[index][\"labels_vect\"], dtype = torch.float32)\n            return embed, targets\n        if self.datatype==\"test\":\n            id = self.df.iloc[index][\"EntryID\"]\n            return embed, id\n        ","metadata":{"execution":{"iopub.status.busy":"2023-05-17T14:52:07.621143Z","iopub.execute_input":"2023-05-17T14:52:07.621515Z","iopub.status.idle":"2023-05-17T14:52:07.634803Z","shell.execute_reply.started":"2023-05-17T14:52:07.621485Z","shell.execute_reply":"2023-05-17T14:52:07.633915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Pytorch Models Architectures","metadata":{}},{"cell_type":"code","source":"class MultiLayerPerceptron(torch.nn.Module):\n\n    def __init__(self, input_dim, num_classes):\n        super(MultiLayerPerceptron, self).__init__()\n\n        self.linear1 = torch.nn.Linear(input_dim, 1012)\n        self.activation1 = torch.nn.ReLU()\n        self.linear2 = torch.nn.Linear(1012, 712)\n        self.activation2 = torch.nn.ReLU()\n        self.linear3 = torch.nn.Linear(712, num_classes)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.activation1(x)\n        x = self.linear2(x)\n        x = self.activation2(x)\n        x = self.linear3(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-05-17T14:52:08.324096Z","iopub.execute_input":"2023-05-17T14:52:08.324529Z","iopub.status.idle":"2023-05-17T14:52:08.334789Z","shell.execute_reply.started":"2023-05-17T14:52:08.324479Z","shell.execute_reply":"2023-05-17T14:52:08.333579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CNN1D(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(CNN1D, self).__init__()\n        # (batch_size, channels, embed_size)\n        self.conv1 = nn.Conv1d(in_channels=1, out_channels=3, kernel_size=3, dilation=1, padding=1, stride=1)\n        # (batch_size, 3, embed_size)\n        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n        # (batch_size, 3, embed_size/2 = 512)\n        self.conv2 = nn.Conv1d(in_channels=3, out_channels=8, kernel_size=3, dilation=1, padding=1, stride=1)\n        # (batch_size, 8, embed_size/2 = 512)\n        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n        # (batch_size, 8, embed_size/4 = 256)\n        self.fc1 = nn.Linear(in_features=int(8 * input_dim/4), out_features=128)\n        self.fc2 = nn.Linear(in_features=128, out_features=num_classes)\n\n    def forward(self, x):\n        x = x.reshape(x.shape[0], 1, x.shape[1])\n        x = self.pool1(nn.functional.relu(self.conv1(x)))\n        x = self.pool2(nn.functional.relu(self.conv2(x)))\n        x = torch.flatten(x, 1)\n        x = nn.functional.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2023-05-17T14:52:08.815831Z","iopub.execute_input":"2023-05-17T14:52:08.816168Z","iopub.status.idle":"2023-05-17T14:52:08.826666Z","shell.execute_reply.started":"2023-05-17T14:52:08.81614Z","shell.execute_reply":"2023-05-17T14:52:08.825362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Train the Model","metadata":{}},{"cell_type":"code","source":"def train_model(embeddings_source, model_type=\"linear\", train_size=0.9):\n    \n    train_dataset = ProteinSequenceDataset(datatype=\"train\", embeddings_source = embeddings_source)\n    \n    train_set, val_set = random_split(train_dataset, lengths = [int(len(train_dataset)*train_size), len(train_dataset)-int(len(train_dataset)*train_size)])\n    train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=config.batch_size, shuffle=True)\n    val_dataloader = torch.utils.data.DataLoader(val_set, batch_size=config.batch_size, shuffle=True)\n\n    if model_type == \"linear\":\n        model = MultiLayerPerceptron(input_dim=embeds_dim[embeddings_source], num_classes=config.num_labels).to(config.device)\n    if model_type == \"convolutional\":\n        model = CNN1D(input_dim=embeds_dim[embeddings_source], num_classes=config.num_labels).to(config.device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr = config.lr)\n    scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=1)\n    CrossEntropy = torch.nn.CrossEntropyLoss()\n    f1_score = MultilabelF1Score(num_labels=config.num_labels).to(config.device)\n    n_epochs = config.n_epochs\n\n    print(\"BEGIN TRAINING...\")\n    train_loss_history=[]\n    val_loss_history=[]\n    \n    train_f1score_history=[]\n    val_f1score_history=[]\n    for epoch in range(n_epochs):\n        print(\"EPOCH \", epoch+1)\n        ## TRAIN PHASE :\n        losses = []\n        scores = []\n        for embed, targets in tqdm(train_dataloader):\n            embed, targets = embed.to(config.device), targets.to(config.device)\n            optimizer.zero_grad()\n            preds = model(embed)\n            loss= CrossEntropy(preds, targets)\n            score=f1_score(preds, targets)\n            losses.append(loss.item()) \n            scores.append(score.item())\n            loss.backward()\n            optimizer.step()\n        avg_loss = np.mean(losses)\n        avg_score = np.mean(scores)\n        print(\"Running Average TRAIN Loss : \", avg_loss)\n        print(\"Running Average TRAIN F1-Score : \", avg_score)\n        train_loss_history.append(avg_loss)\n        train_f1score_history.append(avg_score)\n        \n        ## VALIDATION PHASE : \n        losses = []\n        scores = []\n        for embed, targets in val_dataloader:\n            embed, targets = embed.to(config.device), targets.to(config.device)\n            preds = model(embed)\n            loss= CrossEntropy(preds, targets)\n            score=f1_score(preds, targets)\n            losses.append(loss.item())\n            scores.append(score.item())\n        avg_loss = np.mean(losses)\n        avg_score = np.mean(scores)\n        print(\"Running Average VAL Loss : \", avg_loss)\n        print(\"Running Average VAL F1-Score : \", avg_score)\n        val_loss_history.append(avg_loss)\n        val_f1score_history.append(avg_score)\n        \n        scheduler.step(avg_loss)\n        print(\"\\n\")\n        \n    print(\"TRAINING FINISHED\")\n    print(\"FINAL TRAINING SCORE : \", train_f1score_history[-1])\n    print(\"FINAL VALIDATION SCORE : \", val_f1score_history[-1])\n    \n    losses_history = {\"train\" : train_loss_history, \"val\" : val_loss_history}\n    scores_history = {\"train\" : train_f1score_history, \"val\" : val_f1score_history}\n    \n    return model, losses_history, scores_history","metadata":{"execution":{"iopub.status.busy":"2023-05-17T14:52:09.776352Z","iopub.execute_input":"2023-05-17T14:52:09.776719Z","iopub.status.idle":"2023-05-17T14:52:09.794621Z","shell.execute_reply.started":"2023-05-17T14:52:09.776689Z","shell.execute_reply":"2023-05-17T14:52:09.793342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7.1. Training Usecase","metadata":{}},{"cell_type":"code","source":"ems2_model, ems2_losses, ems2_scores = train_model(embeddings_source=\"EMS2\",model_type=\"linear\")","metadata":{"execution":{"iopub.status.busy":"2023-05-17T14:52:11.100958Z","iopub.execute_input":"2023-05-17T14:52:11.101636Z","iopub.status.idle":"2023-05-17T14:54:40.639291Z","shell.execute_reply.started":"2023-05-17T14:52:11.101593Z","shell.execute_reply":"2023-05-17T14:54:40.638164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t5_model, t5_losses, t5_scores = train_model(embeddings_source=\"T5\",model_type=\"linear\")","metadata":{"execution":{"iopub.status.busy":"2023-05-17T14:55:25.520425Z","iopub.execute_input":"2023-05-17T14:55:25.521455Z","iopub.status.idle":"2023-05-17T14:57:48.225494Z","shell.execute_reply.started":"2023-05-17T14:55:25.521402Z","shell.execute_reply":"2023-05-17T14:57:48.224439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"protbert_model, protbert_losses, protbert_scores = train_model(embeddings_source=\"T5\",model_type=\"linear\")","metadata":{"execution":{"iopub.status.busy":"2023-05-17T14:57:51.426026Z","iopub.execute_input":"2023-05-17T14:57:51.426707Z","iopub.status.idle":"2023-05-17T15:00:02.604399Z","shell.execute_reply.started":"2023-05-17T14:57:51.42666Z","shell.execute_reply":"2023-05-17T15:00:02.603425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7.3. Train/Val Losses ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (10, 4))\nplt.plot(ems2_losses[\"val\"], label = \"EMS2\")\nplt.plot(t5_losses[\"val\"], label = \"T5\")\nplt.plot(protbert_losses[\"val\"], label = \"ProtBERT\")\nplt.title(\"Validation Losses for # Vector Embeddings\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Average Loss\")\nplt.legend()\nplt.show()\n\nplt.figure(figsize = (10, 4))\nplt.plot(ems2_scores[\"val\"], label = \"EMS2\")\nplt.plot(t5_scores[\"val\"], label = \"T5\")\nplt.plot(protbert_scores[\"val\"], label = \"ProtBERT\")\nplt.title(\"Validation F1-Scores for # Vector Embeddings\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Average F1-Score\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-17T15:00:39.421321Z","iopub.execute_input":"2023-05-17T15:00:39.422073Z","iopub.status.idle":"2023-05-17T15:00:39.994213Z","shell.execute_reply.started":"2023-05-17T15:00:39.422038Z","shell.execute_reply":"2023-05-17T15:00:39.993298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Pytorch Lightning module","metadata":{}},{"cell_type":"code","source":"class Linear_Lightning(pl.LightningModule):\n    def __init__(self, input_dim, num_classes, train_size, **hparams):\n        super(Linear_Lightning, self).__init__()\n        self.linear1 = torch.nn.Linear(input_dim, 1012)\n        self.activation1 = torch.nn.ReLU()\n        self.linear2 = torch.nn.Linear(1012, 712)\n        self.activation2 = torch.nn.ReLU()\n        self.linear3 = torch.nn.Linear(712, num_classes)\n        \n        train_dataset = ProteinSequenceDataset(datatype=\"train\", embeddings_source = embeddings_source)\n        self.train_set, self.val_set = random_split(train_dataset, lengths = [int(len(train_dataset)*train_size), len(train_dataset)-int(len(train_dataset)*train_size)])\n        self.loss_fn = torch.nn.CrossEntropyLoss()\n        \n        self.batch_size = batch_size\n        self.lr = lr\n        \n        self.f1_score = MultilabelF1Score(num_labels=num_classes)\n        self.accuracy = MultilabelAccuracy(num_labels=num_classes)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.activation1(x)\n        x = self.linear2(x)\n        x = self.activation2(x)\n        x = self.linear3(x)\n        return x\n    \n    def training_step(self, batch, batch_idx):\n        embed, targets = batch\n        preds = self(embed)\n        loss = self.loss_fn(preds, targets)\n        f1_score = self.f1_score(preds, targets)\n        acc_score = self.accuracy(preds, targets)\n        \n        logs = {\"train_loss\" : loss, \"f1_score\" : f1_score, \"accuracy_score\" : acc_score}\n        self.log_dict(\n            logs,\n            on_step=True, on_epoch=True, prog_bar=True, logger=True\n        )\n        return {\"loss\":loss, \"log\" : logs}\n    \n    def validation_step(self, batch, batch_idx):\n        embed, targets = batch\n        preds = self(embed)\n        loss= self.loss_fn(preds, targets)\n        f1_score = self.f1_score(preds, targets)\n        acc_score = self.accuracy(preds, targets)\n        \n        return {\"val_loss\":loss, \"f1_score\" : f1_score, \"accuracy_score\" : acc_score}\n    \n    def validation_end(self, outputs):\n        avg_loss = torch.stack([x[\"val_loss\"] for x in ouputs]).mean()\n        logs = {\"val_loss\" : avg_loss}\n        self.log_dict(\n            logs,\n            on_step=True, on_epoch=True, prog_bar=True, logger=True\n        )\n        return {\"avg_val_loss\" : avg_loss, \"log\" : logs}\n        \n    def val_dataloader(self):\n        val_dataloader = torch.utils.data.DataLoader(self.val_set, batch_size=config.batch_size, shuffle=False,)\n        return val_dataloader\n\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n        return optimizer\n    \n    def train_dataloader(self):\n        train_dataloader = torch.utils.data.DataLoader(self.train_set, batch_size=self.batch_size, shuffle=False)\n        return train_dataloader","metadata":{"execution":{"iopub.status.busy":"2023-05-17T15:00:58.961491Z","iopub.execute_input":"2023-05-17T15:00:58.96191Z","iopub.status.idle":"2023-05-17T15:00:58.981314Z","shell.execute_reply.started":"2023-05-17T15:00:58.961876Z","shell.execute_reply":"2023-05-17T15:00:58.980487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## IN PROGRESS\n```python\nparam_grid = {\n    \"batch_size\" : [8, 16, 32, 64],\n    \"lr\" : [1e-4, 1e-3, 1e-2]\n}\n\nembeddings_source = \"T5\"\n\nruns_count=0\nfor batch_size in param_grid[\"batch_size\"]:\n    for lr in param_grid[\"lr\"]:\n        runs_count+=1\n        print(\"NEW SESSION RUN (number \"+str(runs_count)+\")\")\n        print(\"batch size : \", batch_size)\n        print(\"learning rate : \", lr)\n        \n        run_name = \"Adam-\"+str(batch_size)+\"-\"+str(lr)\n        \n        logger = WandbLogger(\n            name = run_name,\n            project=\"mlp_model_cafa5\",\n            job_type=\"train\"\n        )\n\n        trainer = Trainer(\n            max_epochs=1,\n            limit_train_batches=5000,\n            logger=logger\n        )\n        model = Linear_Lightning(\n            input_dim=embeds_dim[embeddings_source],\n            num_classes=config.num_labels,\n            train_size=0.8\n        )\n\n        trainer.fit(model)\n```","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:06:26.806291Z","iopub.execute_input":"2023-05-11T21:06:26.807432Z","iopub.status.idle":"2023-05-11T21:06:30.034869Z","shell.execute_reply.started":"2023-05-11T21:06:26.807367Z","shell.execute_reply":"2023-05-11T21:06:30.033213Z"}}},{"cell_type":"markdown","source":"```python\n# Define sweep config\nsweep_configuration = {\n    'method': 'random',\n    'name': 'sweep',\n    'metric': {'goal': 'maximize', 'name': 'val_acc'},\n    'parameters': \n    {\n        'batch_size': {'values': [16, 32, 64]},\n        'epochs': {'values': [5, 10, 15]},\n        'lr': {'max': 0.1, 'min': 0.0001}\n     }\n}\n\n# Initialize sweep by passing in config. \n# (Optional) Provide a name of the project.\nsweep_id = wandb.sweep(\n  sweep=sweep_configuration, \n  project='my-first-sweep'\n  )\n\n# Define training function that takes in hyperparameter \n# values from `wandb.config` and uses them to train a \n# model and return metric\ndef train_one_epoch(epoch, lr, bs): \n  acc = 0.25 + ((epoch/30) +  (random.random()/10))\n  loss = 0.2 + (1 - ((epoch-1)/10 +  random.random()/5))\n  return acc, loss\n\ndef evaluate_one_epoch(epoch): \n  acc = 0.1 + ((epoch/20) +  (random.random()/10))\n  loss = 0.25 + (1 - ((epoch-1)/10 +  random.random()/6))\n  return acc, loss\n\ndef main():\n    run = wandb.init()\n\n    lr  =  wandb.config.lr\n    bs = wandb.config.batch_size\n    epochs = wandb.config.epochs\n\n    for epoch in np.arange(1, epochs):\n      train_acc, train_loss = train_one_epoch(epoch, lr, bs)\n      val_acc, val_loss = evaluate_one_epoch(epoch)\n\n      wandb.log({\n        'epoch': epoch, \n        'train_acc': train_acc,\n        'train_loss': train_loss, \n        'val_acc': val_acc, \n        'val_loss': val_loss\n      })\n\n# Start sweep job.\nwandb.agent(sweep_id, function=main, count=4)\n```","metadata":{"_kg_hide-input":true}},{"cell_type":"markdown","source":"# 9. Make Predictions","metadata":{}},{"cell_type":"code","source":"def predict(embeddings_source):\n    \n    test_dataset = ProteinSequenceDataset(datatype=\"test\", embeddings_source = embeddings_source)\n    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n    \n    if embeddings_source == \"T5\":\n        model = t5_model\n    if embeddings_source == \"ProtBERT\":\n        model = protbert_model\n    if embeddings_source == \"EMS2\":\n        model = ems2_model\n        \n    model.eval()\n    \n    labels = pd.read_csv(config.train_labels_path, sep = \"\\t\")\n    top_terms = labels.groupby(\"term\")[\"EntryID\"].count().sort_values(ascending=False)\n    labels_names = top_terms[:config.num_labels].index.values\n    print(\"GENERATE PREDICTION FOR TEST SET...\")\n\n    ids_ = np.empty(shape=(len(test_dataloader)*config.num_labels,), dtype=object)\n    go_terms_ = np.empty(shape=(len(test_dataloader)*config.num_labels,), dtype=object)\n    confs_ = np.empty(shape=(len(test_dataloader)*config.num_labels,), dtype=np.float32)\n\n    for i, (embed, id) in tqdm(enumerate(test_dataloader)):\n        embed = embed.to(config.device)\n        confs_[i*config.num_labels:(i+1)*config.num_labels] = torch.nn.functional.sigmoid(model(embed)).squeeze().detach().cpu().numpy()\n        ids_[i*config.num_labels:(i+1)*config.num_labels] = id[0]\n        go_terms_[i*config.num_labels:(i+1)*config.num_labels] = labels_names\n\n    submission_df = pd.DataFrame(data={\"Id\" : ids_, \"GO term\" : go_terms_, \"Confidence\" : confs_})\n    print(\"PREDICTIONS DONE\")\n    return submission_df","metadata":{"execution":{"iopub.status.busy":"2023-05-17T15:01:02.864457Z","iopub.execute_input":"2023-05-17T15:01:02.86481Z","iopub.status.idle":"2023-05-17T15:01:02.878954Z","shell.execute_reply.started":"2023-05-17T15:01:02.86478Z","shell.execute_reply":"2023-05-17T15:01:02.878081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = predict(\"T5\")","metadata":{"execution":{"iopub.status.busy":"2023-05-17T15:01:07.621698Z","iopub.execute_input":"2023-05-17T15:01:07.622698Z","iopub.status.idle":"2023-05-17T15:02:57.475262Z","shell.execute_reply.started":"2023-05-17T15:01:07.622649Z","shell.execute_reply":"2023-05-17T15:02:57.474269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.head(50)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T15:02:57.477305Z","iopub.execute_input":"2023-05-17T15:02:57.478338Z","iopub.status.idle":"2023-05-17T15:02:57.497013Z","shell.execute_reply.started":"2023-05-17T15:02:57.478302Z","shell.execute_reply":"2023-05-17T15:02:57.495709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(submission_df)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T15:02:57.498329Z","iopub.execute_input":"2023-05-17T15:02:57.498741Z","iopub.status.idle":"2023-05-17T15:02:57.506305Z","shell.execute_reply.started":"2023-05-17T15:02:57.498709Z","shell.execute_reply":"2023-05-17T15:02:57.505428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('submission.tsv', sep='\\t', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T15:02:57.508569Z","iopub.execute_input":"2023-05-17T15:02:57.509124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### *I appreciate any feedback or Upvote as support*\n### *Also open for teaming, contact me in comments or mp*","metadata":{}}]}